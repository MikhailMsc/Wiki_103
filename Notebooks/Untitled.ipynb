{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5517505f-4924-4544-b216-69cd79a20068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import psutil\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter, OrderedDict, namedtuple\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import einops\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "from random import randint\n",
    "import random\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PROJECT_DIR = Path().absolute().parent\n",
    "WIKI_PATH = PROJECT_DIR / 'InputData' / 'wikitext-103'\n",
    "DATA_PATH = PROJECT_DIR / 'Data'\n",
    "\n",
    "sys.path.append(str(PROJECT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a8620f7-5ec0-44aa-b4e6-bef4f8d2f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f'{round(process.memory_info().rss * 10**(-6))} MB')\n",
    "\n",
    "def objects_memory(*args):\n",
    "    print(f'{round(sum([sys.getsizeof(obj) for obj in args]) * 10**(-6))} MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc170623-263d-42cb-8382-a59ec16f40cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(obj, pth):\n",
    "    with open(pth, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def load_pkl(pth):\n",
    "    with open(pth, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f92c814-b7aa-4203-8bfb-f01e1853f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    Regex,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5515c0ce-5c5a-4880-983c-a5b8b6b6bf48",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aba1617-5b15-47d3-8ab4-1b54e7bafe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(tp='train'):\n",
    "    if tp not in ['train', 'test', 'valid']:\n",
    "        raise Exception('ERROR: Wrong type of data.')\n",
    "    \n",
    "    pth = WIKI_PATH / f'wiki.{tp}.raw'\n",
    "    heading_pattern = '\\n (= ){1,}[^=]*[^=] (= ){1,}\\n \\n'\n",
    "    with open(pth, 'r') as f:\n",
    "        raw_text = f.read()\n",
    "    \n",
    "    raw_text = re.split(heading_pattern, raw_text)\n",
    "    raw_text = [x.strip().strip('\\n').strip() for x in raw_text if x and x not in [' ', '= ']]\n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe72d18-3d98-4d38-be4b-a73ffb80c0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271821/623/552\n",
      "CPU times: user 4.41 s, sys: 2.55 s, total: 6.96 s\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = load_data('train')\n",
    "test_data = load_data('test')\n",
    "valid_data = load_data('valid')\n",
    "\n",
    "print(f'{len(train_data)}/{len(test_data)}/{len(valid_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "236eddb0-6ab8-43fe-a858-2bc500c94fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    for i in range(0, len(train_data), 1000):\n",
    "        yield train_data[i : i + 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30db5bc-102f-4cc7-bc21-2903c15e7d45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2e71ab6-87b4-4a15-8d2e-3d987cba56ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_tokenizer = Tokenizer(models.Unigram())\n",
    "tmp_tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.Replace(\"”\", '\"'),\n",
    "        normalizers.Replace(\"“\", '\"'),\n",
    "        normalizers.Replace('ˈ', \"'\"),\n",
    "        normalizers.Replace('’',\"'\"),\n",
    "        normalizers.Replace('–',\"-\"),\n",
    "        normalizers.Replace('—',\"-\"),\n",
    "        normalizers.Replace('−',\"-\"),\n",
    "        normalizers.Replace('′',\"'\"),\n",
    "        normalizers.Replace('⁄',\"/\"),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")\n",
    "tmp_tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n",
    "    [\n",
    "        pre_tokenizers.BertPreTokenizer(), \n",
    "        # pre_tokenizers.Metaspace(replacement = '_', add_prefix_space = True),\n",
    "        # pre_tokenizers.Punctuation(),\n",
    "        pre_tokenizers.Digits(individual_digits=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07969638-8cd6-41f9-9eee-25cbb5f56647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 271821/271821 [00:26<00:00, 10241.82it/s]\n"
     ]
    }
   ],
   "source": [
    "char_counter = Counter()\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    art_counter = Counter(train_data[i])\n",
    "    char_counter.update(art_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01dc6e55-eb0c-4963-a684-2683527d2211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter_df = pd.DataFrame(char_counter.most_common(), columns=['Symbol', 'Count'])\n",
    "char_counter_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d427827-69ad-4710-a0da-57da437268dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4735\n",
       "0     229\n",
       "3      11\n",
       "2       2\n",
       "4       1\n",
       "Name: Base_Symbol, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_base_char(txt):\n",
    "    tokens = tmp_tokenizer.encode(txt).tokens\n",
    "    return tokens\n",
    "char_counter_df['Base_Symbol'] = char_counter_df['Symbol'].apply(get_base_char)\n",
    "char_counter_df['Base_Symbol'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e1ff2ad-61ad-4635-b16b-e4c1cc40047a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Count</th>\n",
       "      <th>Base_Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>…</td>\n",
       "      <td>1961</td>\n",
       "      <td>[., ., .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>½</td>\n",
       "      <td>757</td>\n",
       "      <td>[1, ⁄, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>″</td>\n",
       "      <td>535</td>\n",
       "      <td>[′, ′]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>⅓</td>\n",
       "      <td>86</td>\n",
       "      <td>[1, ⁄, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>⅔</td>\n",
       "      <td>58</td>\n",
       "      <td>[2, ⁄, 3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>¼</td>\n",
       "      <td>55</td>\n",
       "      <td>[1, ⁄, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>¾</td>\n",
       "      <td>55</td>\n",
       "      <td>[3, ⁄, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>⅜</td>\n",
       "      <td>11</td>\n",
       "      <td>[3, ⁄, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>⅛</td>\n",
       "      <td>9</td>\n",
       "      <td>[1, ⁄, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>⅝</td>\n",
       "      <td>9</td>\n",
       "      <td>[5, ⁄, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>⅞</td>\n",
       "      <td>3</td>\n",
       "      <td>[7, ⁄, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2599</th>\n",
       "      <td>‼</td>\n",
       "      <td>3</td>\n",
       "      <td>[!, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3359</th>\n",
       "      <td>ﷺ</td>\n",
       "      <td>2</td>\n",
       "      <td>[صلى, الله, عليه, وسلم]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>⅙</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, ⁄, 6]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol  Count              Base_Symbol\n",
       "122       …   1961                [., ., .]\n",
       "172       ½    757                [1, ⁄, 2]\n",
       "202       ″    535                   [′, ′]\n",
       "468       ⅓     86                [1, ⁄, 3]\n",
       "568       ⅔     58                [2, ⁄, 3]\n",
       "591       ¼     55                [1, ⁄, 4]\n",
       "592       ¾     55                [3, ⁄, 4]\n",
       "1310      ⅜     11                [3, ⁄, 8]\n",
       "1468      ⅛      9                [1, ⁄, 8]\n",
       "1482      ⅝      9                [5, ⁄, 8]\n",
       "2580      ⅞      3                [7, ⁄, 8]\n",
       "2599      ‼      3                   [!, !]\n",
       "3359      ﷺ      2  [صلى, الله, عليه, وسلم]\n",
       "4238      ⅙      1                [1, ⁄, 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter_df[char_counter_df['Base_Symbol'].apply(len) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cacd9e1-f01a-4fe8-aff5-808084e32c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Count</th>\n",
       "      <th>Base_Symbol</th>\n",
       "      <th>Single_Base_Symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>99530965</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>48657548</td>\n",
       "      <td>[e]</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t</td>\n",
       "      <td>33788437</td>\n",
       "      <td>[t]</td>\n",
       "      <td>t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>33364371</td>\n",
       "      <td>[a]</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n</td>\n",
       "      <td>28965321</td>\n",
       "      <td>[n]</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>課</td>\n",
       "      <td>1</td>\n",
       "      <td>[課]</td>\n",
       "      <td>課</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>純</td>\n",
       "      <td>1</td>\n",
       "      <td>[純]</td>\n",
       "      <td>純</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>丽</td>\n",
       "      <td>1</td>\n",
       "      <td>[丽]</td>\n",
       "      <td>丽</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>치</td>\n",
       "      <td>1</td>\n",
       "      <td>[치]</td>\n",
       "      <td>치</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>킨</td>\n",
       "      <td>1</td>\n",
       "      <td>[킨]</td>\n",
       "      <td>킨</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4978 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol     Count Base_Symbol Single_Base_Symbol\n",
       "0            99530965          []                   \n",
       "1         e  48657548         [e]                  e\n",
       "2         t  33788437         [t]                  t\n",
       "3         a  33364371         [a]                  a\n",
       "4         n  28965321         [n]                  n\n",
       "...     ...       ...         ...                ...\n",
       "4973      課         1         [課]                  課\n",
       "4974      純         1         [純]                  純\n",
       "4975      丽         1         [丽]                  丽\n",
       "4976      치         1        [치]                 치\n",
       "4977      킨         1       [킨]                킨\n",
       "\n",
       "[4978 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter_df['Single_Base_Symbol'] = char_counter_df['Base_Symbol'].apply(lambda x: x[0] if x else '')\n",
    "char_counter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44eb273f-0f55-4690-87e2-5cbeeec121ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single_Base_Symbol</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>100128792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e</td>\n",
       "      <td>48715319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t</td>\n",
       "      <td>33789131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>33401452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n</td>\n",
       "      <td>28970107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>恢</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>恒</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>恆</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>怨</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>🖕</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Single_Base_Symbol      Count\n",
       "0                        100128792\n",
       "1                     e   48715319\n",
       "2                     t   33789131\n",
       "3                     a   33401452\n",
       "4                     n   28970107\n",
       "...                 ...        ...\n",
       "4195                  恢          1\n",
       "4196                  恒          1\n",
       "4197                  恆          1\n",
       "4198                  怨          1\n",
       "4199                  🖕          1\n",
       "\n",
       "[4200 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter_df = char_counter_df.groupby('Single_Base_Symbol')['Count'].sum().reset_index()\n",
    "char_counter_df = char_counter_df.sort_values('Count', ascending=False).reset_index(drop=True)\n",
    "char_counter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a97a3d22-1d75-4502-bf12-850f23190881",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_counter_df['Cum_Prc'] = (char_counter_df['Count'] / char_counter_df['Count'].sum()).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c1cc5343-4f8f-4f82-9b33-ccadab7888b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Single_Base_Symbol</th>\n",
       "      <th>Count</th>\n",
       "      <th>Cum_Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>)</td>\n",
       "      <td>572467</td>\n",
       "      <td>0.984419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>(</td>\n",
       "      <td>572111</td>\n",
       "      <td>0.985501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>541764</td>\n",
       "      <td>0.986526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5</td>\n",
       "      <td>538413</td>\n",
       "      <td>0.987544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>8</td>\n",
       "      <td>532060</td>\n",
       "      <td>0.988550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>E</td>\n",
       "      <td>514159</td>\n",
       "      <td>0.989523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>J</td>\n",
       "      <td>503445</td>\n",
       "      <td>0.990475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>495231</td>\n",
       "      <td>0.991412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>O</td>\n",
       "      <td>462139</td>\n",
       "      <td>0.992286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>6</td>\n",
       "      <td>445885</td>\n",
       "      <td>0.993129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>7</td>\n",
       "      <td>442039</td>\n",
       "      <td>0.993965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>z</td>\n",
       "      <td>418318</td>\n",
       "      <td>0.994757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>j</td>\n",
       "      <td>379936</td>\n",
       "      <td>0.995475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>K</td>\n",
       "      <td>347264</td>\n",
       "      <td>0.996132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>q</td>\n",
       "      <td>339468</td>\n",
       "      <td>0.996774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>U</td>\n",
       "      <td>311682</td>\n",
       "      <td>0.997364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>V</td>\n",
       "      <td>254111</td>\n",
       "      <td>0.997844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>;</td>\n",
       "      <td>190871</td>\n",
       "      <td>0.998205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>:</td>\n",
       "      <td>176460</td>\n",
       "      <td>0.998539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Y</td>\n",
       "      <td>144573</td>\n",
       "      <td>0.998812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>/</td>\n",
       "      <td>72465</td>\n",
       "      <td>0.998949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Z</td>\n",
       "      <td>56234</td>\n",
       "      <td>0.999056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>$</td>\n",
       "      <td>46731</td>\n",
       "      <td>0.999144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Q</td>\n",
       "      <td>46310</td>\n",
       "      <td>0.999232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>]</td>\n",
       "      <td>39560</td>\n",
       "      <td>0.999307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>[</td>\n",
       "      <td>39478</td>\n",
       "      <td>0.999381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>%</td>\n",
       "      <td>38496</td>\n",
       "      <td>0.999454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>X</td>\n",
       "      <td>38489</td>\n",
       "      <td>0.999527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>26405</td>\n",
       "      <td>0.999577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>!</td>\n",
       "      <td>17692</td>\n",
       "      <td>0.999610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>°</td>\n",
       "      <td>13468</td>\n",
       "      <td>0.999636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>£</td>\n",
       "      <td>12026</td>\n",
       "      <td>0.999659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>?</td>\n",
       "      <td>11922</td>\n",
       "      <td>0.999681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>+</td>\n",
       "      <td>7602</td>\n",
       "      <td>0.999695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>#</td>\n",
       "      <td>4763</td>\n",
       "      <td>0.999704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>ø</td>\n",
       "      <td>4750</td>\n",
       "      <td>0.999713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>=</td>\n",
       "      <td>4337</td>\n",
       "      <td>0.999722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>ł</td>\n",
       "      <td>4112</td>\n",
       "      <td>0.999729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>&gt;</td>\n",
       "      <td>3857</td>\n",
       "      <td>0.999737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>&lt;</td>\n",
       "      <td>3754</td>\n",
       "      <td>0.999744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>×</td>\n",
       "      <td>3633</td>\n",
       "      <td>0.999751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>α</td>\n",
       "      <td>2819</td>\n",
       "      <td>0.999756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Æ</td>\n",
       "      <td>2759</td>\n",
       "      <td>0.999761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>μ</td>\n",
       "      <td>2739</td>\n",
       "      <td>0.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>*</td>\n",
       "      <td>2733</td>\n",
       "      <td>0.999772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ο</td>\n",
       "      <td>2260</td>\n",
       "      <td>0.999776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>æ</td>\n",
       "      <td>2251</td>\n",
       "      <td>0.999780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ð</td>\n",
       "      <td>2076</td>\n",
       "      <td>0.999784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>₹</td>\n",
       "      <td>1747</td>\n",
       "      <td>0.999787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ə</td>\n",
       "      <td>1694</td>\n",
       "      <td>0.999791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Đ</td>\n",
       "      <td>1617</td>\n",
       "      <td>0.999794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>ι</td>\n",
       "      <td>1578</td>\n",
       "      <td>0.999797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>ː</td>\n",
       "      <td>1483</td>\n",
       "      <td>0.999799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>ν</td>\n",
       "      <td>1327</td>\n",
       "      <td>0.999802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>ε</td>\n",
       "      <td>1303</td>\n",
       "      <td>0.999804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>ς</td>\n",
       "      <td>1268</td>\n",
       "      <td>0.999807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>а</td>\n",
       "      <td>1238</td>\n",
       "      <td>0.999809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>и</td>\n",
       "      <td>1190</td>\n",
       "      <td>0.999811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>τ</td>\n",
       "      <td>1144</td>\n",
       "      <td>0.999814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>€</td>\n",
       "      <td>1097</td>\n",
       "      <td>0.999816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Single_Base_Symbol   Count   Cum_Prc\n",
       "50                   )  572467  0.984419\n",
       "51                   (  572111  0.985501\n",
       "52                   3  541764  0.986526\n",
       "53                   5  538413  0.987544\n",
       "54                   8  532060  0.988550\n",
       "55                   E  514159  0.989523\n",
       "56                   J  503445  0.990475\n",
       "57                   4  495231  0.991412\n",
       "58                   O  462139  0.992286\n",
       "59                   6  445885  0.993129\n",
       "60                   7  442039  0.993965\n",
       "61                   z  418318  0.994757\n",
       "62                   j  379936  0.995475\n",
       "63                   K  347264  0.996132\n",
       "64                   q  339468  0.996774\n",
       "65                   U  311682  0.997364\n",
       "66                   V  254111  0.997844\n",
       "67                   ;  190871  0.998205\n",
       "68                   :  176460  0.998539\n",
       "69                   Y  144573  0.998812\n",
       "70                   /   72465  0.998949\n",
       "71                   Z   56234  0.999056\n",
       "72                   $   46731  0.999144\n",
       "73                   Q   46310  0.999232\n",
       "74                   ]   39560  0.999307\n",
       "75                   [   39478  0.999381\n",
       "76                   %   38496  0.999454\n",
       "77                   X   38489  0.999527\n",
       "78                   &   26405  0.999577\n",
       "79                   !   17692  0.999610\n",
       "80                   °   13468  0.999636\n",
       "81                   £   12026  0.999659\n",
       "82                   ?   11922  0.999681\n",
       "83                   +    7602  0.999695\n",
       "84                   #    4763  0.999704\n",
       "85                   ø    4750  0.999713\n",
       "86                   =    4337  0.999722\n",
       "87                   ł    4112  0.999729\n",
       "88                   >    3857  0.999737\n",
       "89                   <    3754  0.999744\n",
       "90                   ×    3633  0.999751\n",
       "91                   α    2819  0.999756\n",
       "92                   Æ    2759  0.999761\n",
       "93                   μ    2739  0.999766\n",
       "94                   *    2733  0.999772\n",
       "95                   ο    2260  0.999776\n",
       "96                   æ    2251  0.999780\n",
       "97                   ð    2076  0.999784\n",
       "98                   ₹    1747  0.999787\n",
       "99                   ə    1694  0.999791\n",
       "100                  Đ    1617  0.999794\n",
       "101                  ι    1578  0.999797\n",
       "102                  ː    1483  0.999799\n",
       "103                  ν    1327  0.999802\n",
       "104                  ε    1303  0.999804\n",
       "105                  ς    1268  0.999807\n",
       "106                  а    1238  0.999809\n",
       "107                  и    1190  0.999811\n",
       "108                  τ    1144  0.999814\n",
       "109                  €    1097  0.999816"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_counter_df.head(110).tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7b6ff5-11ae-4157-9550-91aa250343af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = list(char_counter_df['Single_Base_Symbol'][:90])\n",
    "[x for x in range(10) if str(x) not in alphabet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410151a1-c3e7-447a-92f2-7e93d27f1d87",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Build Tokenizer\n",
    "https://colab.research.google.com/github/tenexcoder/huggingface-tutorials/blob/main/BERT_tokenizer_from_scratch.ipynb\n",
    "https://huggingface.co/transformers/v3.5.1/main_classes/tokenizer.html\n",
    "https://huggingface.co/course/chapter6/8?fw=tf\n",
    "\n",
    "Steps\n",
    "1) Normalization\n",
    "2) Pre_tokenization\n",
    "3) Model\n",
    "4) Post-processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e2b68-83df-41d7-8ab7-30f3013b81ee",
   "metadata": {},
   "source": [
    "## 1) Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a680bc39-74be-4597-8823-e15e0c741ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normlzr = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.Replace(\"”\", '\"'),\n",
    "        normalizers.Replace(\"“\", '\"'),\n",
    "        normalizers.Replace('ˈ', \"'\"),\n",
    "        normalizers.Replace('’',\"'\"),\n",
    "        normalizers.Replace('–',\"-\"),\n",
    "        normalizers.Replace('—',\"-\"),\n",
    "        normalizers.Replace('−',\"-\"),\n",
    "        normalizers.Replace('′',\"'\"),\n",
    "        normalizers.Replace('⁄',\"/\"),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d100011-4790-4961-8217-e97d4fa420a7",
   "metadata": {},
   "source": [
    "## 2) Pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d1a279-382a-4964-8a31-8aac25803ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretknzr = pre_tokenizers.Sequence(\n",
    "    [\n",
    "        pre_tokenizers.BertPreTokenizer(), \n",
    "        # pre_tokenizers.Metaspace(replacement = '_', add_prefix_space = True),\n",
    "        # pre_tokenizers.Punctuation(),\n",
    "        pre_tokenizers.Digits(individual_digits=True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f01ec3-fe13-4df7-bad4-9874ca9d9033",
   "metadata": {},
   "source": [
    "## 3) Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0229a5f-c047-4437-9f3a-d772181b3050",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ?trainers.BpeTrainer\n",
    "# ?trainers.WordPieceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b80a932-637b-4d50-8424-08dc7d566691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'BPE'\n",
    "# model_type = 'WordPiece'\n",
    "\n",
    "SPEC_TOKENS = [\"[UNK]\", \"[PAD]\"]\n",
    "\n",
    "if model_type == 'WordPiece':\n",
    "    tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    trainer = trainers.WordPieceTrainer(\n",
    "        vocab_size=50000, \n",
    "        min_frequency=0, \n",
    "        special_tokens=SPEC_TOKENS, \n",
    "        limit_alphabet=len(alphabet),\n",
    "        initial_alphabet=alphabet,\n",
    "        continuing_subword_prefix='##',\n",
    "        end_of_word_suffix='__'\n",
    "        \n",
    "    )\n",
    "elif model_type == 'BPE':\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=100000, \n",
    "        min_frequency=0, \n",
    "        special_tokens=SPEC_TOKENS, \n",
    "        limit_alphabet=len(alphabet),\n",
    "        initial_alphabet=alphabet,\n",
    "        continuing_subword_prefix='##',\n",
    "        end_of_word_suffix='__'\n",
    "    )\n",
    "    \n",
    "tokenizer.normalizer = normlzr\n",
    "tokenizer.pre_tokenizer = pretknzr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d14bf98f-bd67-4967-a82e-466c1cae8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7abd1009-7fb5-422c-bd99-73fc8ba0767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "    [\n",
    "        normalizers.Replace(\"``\", '\"'),\n",
    "        normalizers.Replace(\"''\", '\"'),\n",
    "        normalizers.Replace(\"”\", '\"'),\n",
    "        normalizers.Replace(\"“\", '\"'),\n",
    "        normalizers.Replace('ˈ', \"'\"),\n",
    "        normalizers.Replace('’',\"'\"),\n",
    "        normalizers.Replace('–',\"-\"),\n",
    "        normalizers.Replace('—',\"-\"),\n",
    "        normalizers.Replace('−',\"-\"),\n",
    "        normalizers.Replace('′',\"'\"),\n",
    "        normalizers.Replace('⁄',\"/\"),\n",
    "        normalizers.NFKD(),\n",
    "        normalizers.StripAccents(),\n",
    "        normalizers.Replace(Regex(\" {2,}\"), \" \"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a38da04f-2a6b-49f3-bd4d-ede08219641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8617af-9873-4060-95ff-0bff18ebe623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "special_tokens = [\"<unk>\", \"<pad>\", \"<s>\", \"</s>\"]\n",
    "trainer = trainers.UnigramTrainer(\n",
    "    vocab_size=25000, special_tokens=special_tokens, unk_token=\"<unk>\"\n",
    ")\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f41423db-08cf-4feb-a499-bbe7d110230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'i', 'zer', '.']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer.\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0dd5180-3bf3-4b3b-a0b8-b8b60ebc7cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'i', 'zer', '.', '.', '.', '▁on', '▁', 'a', '▁pair', '▁of', '▁sentence', 's', '!']\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer...\", \"on a pair of sentences!\")\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97e98f64-1b25-4aaa-a121-1b4ec33e36e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "df7f1f2d-3177-476d-8364-9b3b169448a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(DATA_PATH / \"unigram_tokenizer.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "94624417-ef45-49b9-9c95-5d94266efa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    padding_side=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63ebbd5f-a44c-4418-9dd0-15c806360700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁let', \"'\", 's', '▁test', '▁this', '▁to', 'ken', 'i', 'zer', '.', '▁', 'i', '▁want', '▁you', '.']\n",
      "[1575, 72, 8, 778, 49, 15, 2883, 30, 3483, 7, 4, 30, 1163, 226, 7]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"Let's test this tokenizer. I want you.\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5485fbbb-93be-415d-b256-a6cbb0e48c9b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game \\'s opening theme was sung by May \\'n . \\n It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game \\'s expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "181df2a7-9ef9-461e-aeac-5b320fffdf6b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁sen', 'jo', '▁no', '▁val', 'k', 'y', 'ria', '▁', '3', '▁', ':', '▁un', 'recorded', '▁chronicle', 's', '▁', '(', '▁japanes', 'e', '▁', ':', '▁', '戦', '場', 'の', 'ウ', 'ァ', 'ル', 'キ', 'ュ', 'リ', 'ア', '3', '▁', ',', '▁lit', '▁', '.', '▁val', 'k', 'y', 'ria', '▁of', '▁the', '▁battlefield', '▁', '3', '▁', ')', '▁', ',', '▁common', 'ly', '▁referre', 'd', '▁to', '▁as', '▁val', 'k', 'y', 'ria', '▁chronicle', 's', '▁iii', '▁outside', '▁japan', '▁', ',', '▁is', '▁', 'a', '▁tactical', '▁role', '▁', '@', '-', '@', '▁playing', '▁video', '▁game', '▁develope', 'd', '▁by', '▁sega', '▁and', '▁media', '.', 'vision', '▁for', '▁the', '▁playstation', '▁portable', '▁', '.', '▁released', '▁in', '▁january', '▁2011', '▁in', '▁japan', '▁', ',', '▁it', '▁is', '▁the', '▁third', '▁game', '▁in', '▁the', '▁val', 'k', 'y', 'ria', '▁series', '▁', '.', '▁employ', 'ing', '▁the', '▁same', '▁fusion', '▁of', '▁tactical', '▁and', '▁real', '▁', '@', '-', '@', '▁time', '▁gameplay', '▁as', '▁its', '▁predecessor', 's', '▁', ',', '▁the', '▁story', '▁runs', '▁parallel', '▁to', '▁the', '▁first', '▁game', '▁and', '▁follows', '▁the', '▁', '\"', '▁name', 'less', '▁', '\"', '▁', ',', '▁', 'a', '▁penal', '▁military', '▁unit', '▁serving', '▁the', '▁nation', '▁of', '▁galli', 'a', '▁during', '▁the', '▁second', '▁europa', 'n', '▁war', '▁who', '▁perform', '▁secret', '▁black', '▁operations', '▁and', '▁are', '▁pitted', '▁against', '▁the', '▁imperial', '▁unit', '▁', '\"', '▁ca', 'lam', 'a', 't', 'y', '▁raven', '▁', '\"', '▁', '.', '▁', '\\n', '▁the', '▁game', '▁began', '▁development', '▁in', '▁2010', '▁', ',', '▁carrying', '▁over', '▁', 'a', '▁large', '▁portion', '▁of', '▁the', '▁work', '▁done', '▁on', '▁val', 'k', 'y', 'ria', '▁chronicle', 's', '▁', 'i', 'i', '▁', '.', '▁while', '▁it', '▁retaine', 'd', '▁the', '▁standard', '▁features', '▁of', '▁the', '▁series', '▁', ',', '▁it', '▁also', '▁under', 'went', '▁multiple', '▁adjustment', 's', '▁', ',', '▁such', '▁as', '▁making', '▁the', '▁game', '▁more', '▁for', 'g', 'iv', 'ing', '▁for', '▁series', '▁newcomer', 's', '▁', '.', '▁character', '▁designer', '▁rai', 't', 'a', '▁', 'h', 'on', 'jo', 'u', '▁and', '▁composer', '▁hit', 'oshi', '▁', 'saki', 'moto', '▁both', '▁returne', 'd', '▁from', '▁previous', '▁', 'e', 'ntries', '▁', ',', '▁along', '▁with', '▁val', 'k', 'y', 'ria', '▁chronicle', 's', '▁', 'i', 'i', '▁director', '▁take', 'shi', '▁', 'o', 'zawa', '▁', '.', '▁', 'a', '▁large', '▁team', '▁of', '▁writers', '▁handle', 'd', '▁the', '▁script', '▁', '.', '▁the', '▁game', \"▁'s\", '▁opening', '▁theme', '▁was', '▁sung', '▁by', '▁may', '▁', \"'\", 'n', '▁', '.', '▁', '\\n', '▁it', '▁met', '▁with', '▁positive', '▁sales', '▁in', '▁japan', '▁', ',', '▁and', '▁was', '▁praise', 'd', '▁by', '▁both', '▁japanes', 'e', '▁and', '▁western', '▁critics', '▁', '.', '▁after', '▁release', '▁', ',', '▁it', '▁receive', 'd', '▁download', 'able', '▁content', '▁', ',', '▁along', '▁with', '▁an', '▁expande', 'd', '▁edition', '▁in', '▁november', '▁of', '▁that', '▁year', '▁', '.', '▁it', '▁was', '▁also', '▁adapted', '▁into', '▁manga', '▁and', '▁an', '▁original', '▁video', '▁animation', '▁series', '▁', '.', '▁due', '▁to', '▁low', '▁sales', '▁of', '▁val', 'k', 'y', 'ria', '▁chronicle', 's', '▁', 'i', 'i', '▁', ',', '▁val', 'k', 'y', 'ria', '▁chronicle', 's', '▁iii', '▁was', '▁not', '▁localize', 'd', '▁', ',', '▁but', '▁', 'a', '▁fan', '▁translation', '▁compatible', '▁with', '▁the', '▁game', \"▁'s\", '▁expande', 'd', '▁edition', '▁was', '▁released', '▁in', '▁2014', '▁', '.', '▁media', '.', 'vision', '▁would', '▁return', '▁to', '▁the', '▁franchise', '▁with', '▁the', '▁development', '▁of', '▁val', 'k', 'y', 'ria', '▁', ':', '▁', 'a', 'zur', 'e', '▁revolution', '▁for', '▁the', '▁playstation', '▁', '4', '▁', '.']\n",
      "479\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(train_data[0])\n",
    "print(encoding.tokens)\n",
    "print(len(encoding.tokens))\n",
    "# print(encoding.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0392e3e-f673-44ff-847e-d50e6d3a7747",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4) Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6195f4ae-eb5c-450d-9878-354d9385739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Let', (0, 3)),\n",
       " (\"'\", (3, 4)),\n",
       " ('s', (4, 5)),\n",
       " ('test', (6, 10)),\n",
       " ('pre', (11, 14)),\n",
       " ('-', (14, 15)),\n",
       " ('tokenization', (15, 27)),\n",
       " ('!', (27, 28)),\n",
       " ('1', (29, 30)),\n",
       " ('2', (30, 31)),\n",
       " ('3', (31, 32))]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pre_tokenizer.pre_tokenize_str(\"Let's test pre-tokenization! 123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "48ced57b-e05b-4f66-97f4-e7f9ad074338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 10min 2s, sys: 37.1 s, total: 10min 39s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5b36721-3f45-47c4-97ea-95547e1eebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello__', 'how__', 'are__', 'u__', '?__']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Héllò hôw are ü?\").tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "762ecdf7-c126-4719-868a-3542a5174d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(str(DATA_PATH / \"Tokenizer_BPE100k.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "382a4dab-b52e-4022-87f6-4608a32ad531",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=str(DATA_PATH / \"Tokenizer_BPE100k.json\"),\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f05dd-c8e9-42ea-875e-53f8b218d8b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Post Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313fca71-0f41-434b-8d4b-aa008333cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_post_tokenize(encoding):\n",
    "    tokens = ['']\n",
    "    tokens_ids = []\n",
    "    for tk, tkid in zip(encoding.tokens, encoding.ids):\n",
    "        if tk == '[UNK]' and tokens == '[UNK]':[-1]\n",
    "            continue\n",
    "        else:\n",
    "            tokens.append(tk)\n",
    "            tokens_ids.append(tkid)\n",
    "        \n",
    "    tokens.pop(0)\n",
    "    \n",
    "    return tokens, tokens_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "61f4caea-904e-45ed-9402-8489682fa68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "eceae6d1-8dd1-4dcd-8468-af393452999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', ['tid', 'value', 'title', 'upper','part', 'w_end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b23d5b24-32b2-4632-98e1-0ea31ad8e02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_VOCAB = {\n",
    "    'First': tokenizer.get_vocab(),\n",
    "    'First_Reverse': {v:k for k,v in tokenizer.get_vocab().items()},\n",
    "    'First_Second': {\n",
    "        tokenizer.token_to_id('[UNK]'): Token(tid=tokenizer.token_to_id('[UNK]'), value='[unk]', \n",
    "                       title=False, upper=False, part=False, w_end=True),\n",
    "        tokenizer.token_to_id('[PAD]'): Token(tid=tokenizer.token_to_id('[PAD]'), value='[pad]',\n",
    "                       title=False, upper=False, part=False, w_end=True)\n",
    "    },\n",
    "    'First_Second_Reverse': {\n",
    "        Token(tid=tokenizer.token_to_id('[UNK]'), value='[unk]',\n",
    "              title=False, upper=False, part=False, w_end=True): tokenizer.token_to_id('[UNK]'), \n",
    "        Token(tid=tokenizer.token_to_id('[PAD]'), value='[pad]',\n",
    "              title=False, upper=False, part=False, w_end=True): tokenizer.token_to_id('[PAD]'), \n",
    "    },\n",
    "    'Second': {\n",
    "        '[unk]': tokenizer.token_to_id('[UNK]'),\n",
    "        '[pad]': tokenizer.token_to_id('[PAD]'),\n",
    "    },\n",
    "    'Second_Reverse': {\n",
    "        tokenizer.token_to_id('[UNK]'): '[unk]',\n",
    "        tokenizer.token_to_id('[PAD]'): '[pad]',\n",
    "    }    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "825faf34-8b0d-4669-8a78-78e36815d6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100000/100000 [00:53<00:00, 1870.71it/s]\n"
     ]
    }
   ],
   "source": [
    "max_new_id = max(ALL_VOCAB['Second_Reverse'].keys())\n",
    "\n",
    "for tk, tkid in tqdm(tokenizer.get_vocab().items()):\n",
    "    if tkid in ALL_VOCAB['First_Second']:\n",
    "        continue\n",
    "        \n",
    "    part = tk[:2] == '##'\n",
    "    w_end = tk[-2:] == '__'\n",
    "    tk = tk.replace('##','').replace('__','')\n",
    "    upper = tk.isupper()\n",
    "    title = tk[0].isupper()\n",
    "    value = tk.lower()\n",
    "    if value in ALL_VOCAB['Second'].keys():\n",
    "        value_id = ALL_VOCAB['Second'][value]\n",
    "    else:\n",
    "        value_id = max(ALL_VOCAB['Second_Reverse'].keys()) + 1\n",
    "        ALL_VOCAB['Second'][value] = value_id\n",
    "        ALL_VOCAB['Second_Reverse'][value_id] = value\n",
    "    \n",
    "    tk = Token(\n",
    "        tid=value_id, value=value, \n",
    "        title=title, upper=upper, part=part, w_end=w_end\n",
    "    )\n",
    "    ALL_VOCAB['First_Second'][tkid] = tk\n",
    "    ALL_VOCAB['First_Second_Reverse'][tk] = tkid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ac3c1ee2-857c-49a0-b42d-0114b6df2330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75463"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALL_VOCAB['Second_Reverse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3d3040c4-ce10-48c1-a47b-7b1acbb53324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75463"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(x.strip('__').strip('##').lower() for x in  tokenizer.get_vocab().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bc0d471e-7442-495b-a3cc-861bd122bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pkl(ALL_VOCAB, DATA_PATH / \"ALLVOCAB_BPE100k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "10cb2758-132d-4590-94b7-ff57664dbdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_txt(txt, tokenizer, vocab):\n",
    "    encoding = tokenizer.encode(txt)\n",
    "    all_ids = [(fid, vocab['First_Second'][fid]) for fid in encoding.ids]\n",
    "    return all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74f1da8a-75a1-4304-89c1-f3e7f72e204a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sen', '##jo__', 'no__', 'Valky', '##ria__', '3__', ':__', 'Un', '##rec', '##orded__', 'Chronicles__', '(__', 'Japanese__', ':__', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '[UNK]', '3__', ',__', 'lit__', '.__', 'Valky', '##ria__', 'of__', 'the__', 'Battlefield__', '3__', ')__', ',__', 'commonly__', 'referred__', 'to__', 'as__', 'Valky', '##ria__', 'Chronicles__', 'III__', 'outside__', 'Japan__', ',__', 'is__', 'a__', 'tactical__', 'role__', '@__', '-__', '@__', 'playing__', 'video__', 'game__', 'developed__', 'by__', 'Sega__', 'and__', 'Media__', '.__', 'Vision__', 'for__', 'the__', 'PlayStation__', 'Portable__', '.__', 'Released__', 'in__', 'January__', '2__', '0__', '1__', '1__', 'in__', 'Japan__', ',__', 'it__', 'is__', 'the__', 'third__', 'game__', 'in__', 'the__', 'Valky', '##ria__', 'series__', '.__', 'Employ', '##ing__', 'the__', 'same__', 'fusion__', 'of__', 'tactical__', 'and__', 'real__', '@__', '-__', '@__', 'time__', 'gameplay__', 'as__', 'its__', 'predecessors__', ',__', 'the__', 'story__', 'runs__', 'parallel__', 'to__', 'the__', 'first__', 'game__', 'and__', 'follows__', 'the__', '\"__', 'Nam', '##eless__', '\"__', ',__', 'a__', 'penal__', 'military__', 'unit__', 'serving__', 'the__', 'nation__', 'of__', 'Gall', '##ia__', 'during__', 'the__', 'Second__', 'Europ', '##an__', 'War__', 'who__', 'perform__', 'secret__', 'black__', 'operations__', 'and__', 'are__', 'pitted__', 'against__', 'the__', 'Imperial__', 'unit__', '\"__', 'Cal', '##am', '##aty__', 'Raven__', '\"__', '.__', 'The__', 'game__', 'began__', 'development__', 'in__', '2__', '0__', '1__', '0__', ',__', 'carrying__', 'over__', 'a__', 'large__', 'portion__', 'of__', 'the__', 'work__', 'done__', 'on__', 'Valky', '##ria__', 'Chronicles__', 'II__', '.__', 'While__', 'it__', 'retained__', 'the__', 'standard__', 'features__', 'of__', 'the__', 'series__', ',__', 'it__', 'also__', 'underwent__', 'multiple__', 'adjustments__', ',__', 'such__', 'as__', 'making__', 'the__', 'game__', 'more__', 'forg', '##iving__', 'for__', 'series__', 'newcomers__', '.__', 'Character__', 'designer__', 'Ra', '##ita__', 'Hon', '##j', '##ou__', 'and__', 'composer__', 'Hit', '##oshi__', 'Sak', '##imoto__', 'both__', 'returned__', 'from__', 'previous__', 'entries__', ',__', 'along__', 'with__', 'Valky', '##ria__', 'Chronicles__', 'II__', 'director__', 'Tak', '##eshi__', 'Oz', '##awa__', '.__', 'A__', 'large__', 'team__', 'of__', 'writers__', 'handled__', 'the__', 'script__', '.__', 'The__', 'game__', \"'__\", 's__', 'opening__', 'theme__', 'was__', 'sung__', 'by__', 'May__', \"'__\", 'n__', '.__', 'It__', 'met__', 'with__', 'positive__', 'sales__', 'in__', 'Japan__', ',__', 'and__', 'was__', 'praised__', 'by__', 'both__', 'Japanese__', 'and__', 'western__', 'critics__', '.__', 'After__', 'release__', ',__', 'it__', 'received__', 'downloadable__', 'content__', ',__', 'along__', 'with__', 'an__', 'expanded__', 'edition__', 'in__', 'November__', 'of__', 'that__', 'year__', '.__', 'It__', 'was__', 'also__', 'adapted__', 'into__', 'manga__', 'and__', 'an__', 'original__', 'video__', 'animation__', 'series__', '.__', 'Due__', 'to__', 'low__', 'sales__', 'of__', 'Valky', '##ria__', 'Chronicles__', 'II__', ',__', 'Valky', '##ria__', 'Chronicles__', 'III__', 'was__', 'not__', 'localized__', ',__', 'but__', 'a__', 'fan__', 'translation__', 'compatible__', 'with__', 'the__', 'game__', \"'__\", 's__', 'expanded__', 'edition__', 'was__', 'released__', 'in__', '2__', '0__', '1__', '4__', '.__', 'Media__', '.__', 'Vision__', 'would__', 'return__', 'to__', 'the__', 'franchise__', 'with__', 'the__', 'development__', 'of__', 'Valky', '##ria__', ':__', 'Az', '##ure__', 'Revolution__', 'for__', 'the__', 'PlayStation__', '4__', '.__']\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(train_data[0])\n",
    "print(encoding.tokens)\n",
    "print(len(encoding.tokens))\n",
    "# print(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ca42ef33-8d8c-4048-aa1a-de9c84e2f5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2786"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(k,v) for k,v in ALL_VOCAB['First_Second'].items() if len(v.value) > 1 and v.upper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a6297f72-ebda-4601-81ac-a20aa9f426ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43762"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(k,v) for k,v in ALL_VOCAB['First_Second'].items() if len(v.value) > 1 and v.title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "376cf150-5a9e-4cf7-9e50-d26537bd44b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17739"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(k,v) for k,v in ALL_VOCAB['First_Second'].items() if v.part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "77a1a4ed-d478-4b44-a982-e0037e69126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78507"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(k,v) for k,v in ALL_VOCAB['First_Second'].items() if v.w_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "18cc3682-d1b9-4501-978f-8f9c4e32ecab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63572"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([(k,v) for k,v in ALL_VOCAB['First_Second'].items() if v.w_end and not v.part])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "5ef5cf10-416b-4bd0-8a7b-4d8ba90757af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ALL_VOCAB['First_Second'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c74b49eb-decd-442e-bd84-7564376c59c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dog'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_VOCAB['First_Reverse'][20335]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a948c23e-da5d-4115-8d7d-e4a3123faa62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20335"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_VOCAB['First']['Dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6c072-8c0a-41b5-b5e8-24e22d065c45",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GLOBAL CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "c21aca44-59e0-43a3-a485-1c9ef971bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Train Parameters\n",
    "CURRENT_SEQ_LEN = 30\n",
    "BATCH_SIZE = 16\n",
    "AGG_ROUNDS = 10\n",
    "CNT_NEGATIVE = 5\n",
    "MAX_SEQ_LEN = 1024\n",
    "\n",
    "# Architecture parameters\n",
    "\n",
    "    # Counts\n",
    "CNT_MEANINGS = 5\n",
    "CAT_SIZES = [55000, 30000, 15000, 5000]\n",
    "    \n",
    "    # Embedings\n",
    "POS_EMB_SIZE = 5\n",
    "TITLE_EMB_SIZE = 5\n",
    "UPPER_EMB_SIZE = 5\n",
    "PART_EMB_SIZE = 5\n",
    "END_EMB_SIZE = 5\n",
    "MEANING_EMB_SIZE = 20\n",
    "CAT_EMB_SIZE = 10\n",
    "\n",
    "\n",
    "\n",
    "    # Main Sizes\n",
    "PREDICT_SIZE = (TITLE_EMB_SIZE + UPPER_EMB_SIZE + PART_EMB_SIZE + END_EMB_SIZE) + CAT_EMB_SIZE + MEANING_EMB_SIZE\n",
    "HIDDEN_SIZE = int(1.5 * PREDICT_SIZE)\n",
    "INPUT_SIZE = POS_EMB_SIZE + TITLE_EMB_SIZE + UPPER_EMB_SIZE + PART_EMB_SIZE + END_EMB_SIZE + CAT_EMB_SIZE + MEANING_EMB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36143e0-a5bc-4423-b9dc-dfda11b34bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31f8fd-9d8a-437b-a3fe-d1df2b28e665",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Make Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1ebc6-97f5-4b4f-824b-4095b1689728",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=str(DATA_PATH / \"Tokenizer_BPE100k.json\"),\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c558ff4-1667-4b8b-86f7-e2afe8004ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_unks(toks, unk=0):\n",
    "    return [tk for i, tk in enumerate(toks) if (tk != unk) or (i == 0) or (toks[i-1] != unk)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ad8e359-24a0-491e-a739-0884717dc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_tokenizer(txt):\n",
    "    tokens = wrapped_tokenizer(txt)['input_ids']\n",
    "    tokens = merge_unks(tokens, wrapped_tokenizer.unk_token_id)\n",
    "    return np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06dd83eb-7670-43a3-b9e6-12fc55626c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sub_seq(tokens, max_len, pad=1):\n",
    "    ln = len(tokens)\n",
    "    if ln < max_len:\n",
    "        return np.array([np.concatenate([tokens, [pad]*(max_len - ln)])])\n",
    "    sub_seqs = []\n",
    "    for i in range(ln - max_len + 1):\n",
    "        sub_seqs.append(tokens[i:i+max_len])\n",
    "    return np.array(sub_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f0a8d73-4d2a-4249-9cb9-f6e9b67955f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 16s, sys: 1min 8s, total: 9min 24s\n",
      "Wall time: 10min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(103596449, 30)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_30T = [\n",
    "    get_all_sub_seq(final_tokenizer(txt), max_len=MAX_SEQ_LEN, pad=wrapped_tokenizer.pad_token_id) \n",
    "    for txt in train_data\n",
    "]\n",
    "train_30T = np.concatenate(train_30T, axis=0)\n",
    "train_30T = np.array(train_30T, np.uintc)\n",
    "train_30T.shape #(103596449, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b85ec1d-52be-46bf-a3ba-12f1e85afe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / 'train_30T.npy', 'wb') as f:\n",
    "    np.save(f, train_30T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a3c89e07-378c-4528-b00a-fe050a96a90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585 MB\n",
      "24863 MB\n"
     ]
    }
   ],
   "source": [
    "process_memory()\n",
    "objects_memory(train_30T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848ddee-f96d-4c5a-8438-d323244a9538",
   "metadata": {},
   "source": [
    "# Load to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cafa0cba-7ae4-4033-9584-20f587e2131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484 MB\n"
     ]
    }
   ],
   "source": [
    "process_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b0c5a7c-95c4-4b99-98cd-50bf9f97a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / 'train_30T.npy', 'rb') as f:\n",
    "    train_tokens = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb6b536-ba94-4362-9443-a725d92f1720",
   "metadata": {},
   "outputs": [],
   "source": [
    "Token = namedtuple('Token', ['tid', 'value', 'title', 'upper','part', 'w_end'])\n",
    "ALL_VOCAB = load_pkl(DATA_PATH / \"ALLVOCAB_BPE100k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010d47ee-1c11-4357-99fd-48e278823222",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=str(DATA_PATH / \"Tokenizer_BPE100k.json\"),\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306756c6-9b68-4566-b448-9ad7de2c831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1415 MB\n"
     ]
    }
   ],
   "source": [
    "process_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4916bf80-2844-4b0b-b636-4db76389bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12432 MB\n"
     ]
    }
   ],
   "source": [
    "objects_memory(train_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c651215-3853-44b5-a3cc-f42a597d8c2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Arhitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3bdb2bc-9eac-4b8d-ba90-9c8b4b39b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) PosEncodind\n",
    "# 2) PropsEmb\n",
    "# 3) TokensEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d12648-a65a-455f-b50b-4e147937623f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Position Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "4817c42b-4fc7-4a6a-999d-e3ba1b40786f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PosEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, rows, emb_size, cnt_repeats, seed=0):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.rows = rows\n",
    "        self.emb_size = emb_size\n",
    "        self.cnt_repeats = cnt_repeats\n",
    "        self.embedding = nn.Embedding(rows, self.emb_size)\n",
    "        self.init_weights(seed)\n",
    "\n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight, gain=1.0)\n",
    "        self.embedding.weight.data = torch.tensor(\n",
    "            np.array(self.embedding.weight.data), \n",
    "            dtype=torch.float32\n",
    "        ).requires_grad_(True)\n",
    "        \n",
    "    def forward(self, batch): # Batch_Sz x SeqLen x SeqLen\n",
    "        t0 = time.time()\n",
    "        batch = self.embedding(batch)\n",
    "        batch = batch.repeat(self.cnt_repeats,1,1,1,1).permute(1,2,3,0,4) # Batch_Sz x SeqLen x SeqLen x (Cnt_Meanings * Cnt_Cats) x PosEmb\n",
    "        print(f'PosEncoding, forward time: {time.time()-t0}')\n",
    "        return batch\n",
    "    \n",
    "    def forward2(self, batch, ptime=False): \n",
    "        # batch = seq_len x seq_len\n",
    "        t0 = time.time()\n",
    "        batch = self.embedding(batch)\n",
    "        if ptime:\n",
    "            print(f'PosEncoding, forward2 time: {time.time()-t0}')\n",
    "        return batch\n",
    "    \n",
    "    \n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt= getattr(optim, opt_type)([self.embedding.weight], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dd08b0-21a5-4d5f-9e13-156a30be0ca8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Properties Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a71ce752-1026-4cbd-b50b-a85727a6ad74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropsEmbeding(nn.Module):\n",
    "\n",
    "    def __init__(self, title, upper, part, w_end, cnt_repeats, seed=0):\n",
    "        super(PropsEmbeding, self).__init__()\n",
    "        self.title = title\n",
    "        self.upper = upper\n",
    "        self.part = part\n",
    "        self.w_end = w_end\n",
    "        self.cnt_repeats = cnt_repeats\n",
    "        \n",
    "        self.title_emb = nn.Embedding(2, self.title)\n",
    "        self.upp_emb = nn.Embedding(2, self.upper)\n",
    "        self.prt_emb = nn.Embedding(2, self.part)\n",
    "        self.end_emb = nn.Embedding(2, self.w_end)\n",
    "        \n",
    "        # FIX ORDER !!!!!!!\n",
    "        self.seq_embs = [self.title_emb, self.upp_emb, self.prt_emb ,self.end_emb]\n",
    "        self.init_weights(seed)\n",
    "\n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        for atr in dir(self):\n",
    "            if atr[-4:] != '_emb':\n",
    "                continue\n",
    "            lay = getattr(self, atr)\n",
    "            nn.init.xavier_uniform_(lay.weight, gain=1.0)\n",
    "            lay.weight.data = torch.tensor(np.array(lay.weight.data), dtype=torch.float32).requires_grad_(True)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, batch): # Extended, Batch_Size x SeqLen x SeqLen x Props (4)\n",
    "        t0 = time.time()\n",
    "        embs_vals = []\n",
    "        \n",
    "        for i, emb_lay in enumerate(self.seq_embs):\n",
    "            if self.fited:\n",
    "                embs_vals.append(emb_lay(batch[:,:,:,i]))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    embs_vals.append(emb_lay(batch[:,:,:,i]))\n",
    "        \n",
    "        embs_vals = torch.cat(embs_vals, dim=-1)\n",
    "        embs_vals = embs_vals.repeat(self.cnt_repeats,1,1,1,1).permute(1,2,3,0,4)\n",
    "        print(f'PropsEmbeding, forward time: {time.time()-t0}')\n",
    "        return embs_vals # Batch_Size x SeqLen x SeqLen x Cnt_Meanings * Cnt_Cats x PropsEmb\n",
    "    \n",
    "    def forward4(self, batch, ptime=False): \n",
    "        # batch = batch x seq_len x props (=4)\n",
    "        t0 = time.time()\n",
    "        embs_vals = []\n",
    "        seq_len = batch.shape[1]\n",
    "        \n",
    "        embs_vals = torch.cat([\n",
    "            emb_lay(batch[:,:,i])\n",
    "            for i, emb_lay in enumerate(self.seq_embs)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        embs_vals = [\n",
    "            emb_lay(batch[:,:,i])\n",
    "            for i, emb_lay in enumerate(self.seq_embs)\n",
    "        ]\n",
    "        \n",
    "        embs_vals = torch.cat(embs_vals, dim=-1)\n",
    "        # batch x seq_len x props_emb\n",
    "        if ptime:\n",
    "            print(f'PropsEmbeding, forward4 time: {time.time()-t0}')\n",
    "        return embs_vals # Batch_Size x SeqLen x SeqLen x Cnt_Meanings * Cnt_Cats x PropsEmb\n",
    "    \n",
    "\n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)([x.weight for x in self.seq_embs], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9401b0b-69c7-496a-8e7f-b7a34504789e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Tokens Embeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ece1c731-8fa1-431b-b3a5-f2c8c2b3f46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokensEmbeding(nn.Module):\n",
    "\n",
    "    def __init__(self, cnt_tokens, categories_sz, cnt_meanings, meaning_emb_sz, cat_emb_sz, seed=0):\n",
    "        super(TokensEmbeding, self).__init__()\n",
    "        self.cnt_tokens = cnt_tokens\n",
    "        self.categories_sz = categories_sz\n",
    "        self.cnt_categories = len(self.categories_sz)\n",
    "        self.cnt_meanings = cnt_meanings\n",
    "        self.meaning_emb_sz = meaning_emb_sz\n",
    "        self.cat_emb_sz = cat_emb_sz\n",
    "        \n",
    "        # Categories Embeding\n",
    "        self.cat_emb = nn.Embedding(self.cnt_categories, self.cat_emb_sz)\n",
    "        \n",
    "        # Tokens Embeding for each Category\n",
    "        self.all_tokens_embeds = []\n",
    "        for i, cat_sz in enumerate(self.categories_sz):\n",
    "            setattr(self, f'token{i}_emb', nn.Embedding(cat_sz, self.cnt_meanings * self.meaning_emb_sz))\n",
    "            self.all_tokens_embeds.append(getattr(self, f'token{i}_emb'))\n",
    "\n",
    "        self.init_weights(seed)\n",
    "        self.init_merges(seed)\n",
    "    \n",
    "    def init_merges(self, seed):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        all_tokens_ids = np.arange(self.cnt_tokens)\n",
    "        for i, cat_sz in enumerate(self.categories_sz):\n",
    "            cat_dict = dict()\n",
    "            while len(cat_dict) != self.cnt_tokens:\n",
    "                cats_ids = np.arange(cat_sz)\n",
    "                all_tokens_ids = [i for i in range(self.cnt_tokens) if i not in cat_dict]\n",
    "                np.random.shuffle(cats_ids)\n",
    "                np.random.shuffle(all_tokens_ids)\n",
    "                indx = slice(0, min(len(all_tokens_ids), cat_sz))\n",
    "                cat_dict.update(zip(all_tokens_ids[indx], cats_ids[indx], ))\n",
    "\n",
    "            setattr(self, f'cat{i}_merge', cat_dict)\n",
    "        \n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        \n",
    "        for lay in self.all_tokens_embeds:\n",
    "            rows = lay.weight.shape[0]\n",
    "            tmp_lay = lay.weight.data.reshape(rows * self.cnt_meanings, self.meaning_emb_sz).type(torch.float32)\n",
    "            nn.init.xavier_uniform_(tmp_lay, gain=1.0)\n",
    "            tmp_lay = tmp_lay.reshape(rows, self.cnt_meanings * self.meaning_emb_sz)\n",
    "            lay.weight.data = tmp_lay.clone().detach().requires_grad_(True)\n",
    "        \n",
    "        \n",
    "        nn.init.xavier_uniform_(self.cat_emb.weight, gain=1.0)\n",
    "        self.cat_emb.weight.data = torch.tensor(\n",
    "            np.array(self.cat_emb.weight.data), \n",
    "            dtype=torch.float32\n",
    "        ).requires_grad_(True)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, batch): # Batch_Size x SeqLen\n",
    "        t0 = time.time()\n",
    "        rows, columns = batch.shape\n",
    "        all_tk_cat = []\n",
    "        \n",
    "        for i, emb_lay in enumerate(self.all_tokens_embeds):\n",
    "            cat_emb = torch.tensor([i]).\\\n",
    "                repeat(rows, columns, self.cnt_meanings).requires_grad_(False) # Batch_Size x SeqLen x Cnt_Meanings x CatEmb\n",
    "            cat_emb = self.cat_emb(cat_emb)\n",
    "\n",
    "            merge_dict = getattr(self, f'cat{i}_merge')\n",
    "\n",
    "            tokens_i = batch.clone().apply_(merge_dict.get)\n",
    "            tokens_i = emb_lay(tokens_i).reshape(rows, columns, self.cnt_meanings, self.meaning_emb_sz)\n",
    "\n",
    "            tk_cat_i = torch.cat([cat_emb, tokens_i], axis=-1) # Batch_Size x SeqLen x Cnt_Meanings x (CatEmb + MeaningEmb)\n",
    "            all_tk_cat.append(tk_cat_i)\n",
    "       \n",
    "        all_tk_cat = torch.cat(all_tk_cat, axis=-2) # Batch_Size x SeqLen x Cnt_Meanings * CntCats x (CatEmb + MeaningEmb)\n",
    "        all_tk_cat = all_tk_cat.repeat(columns, 1,1,1,1)\n",
    "        all_tk_cat = all_tk_cat.permute(1,2,0,3,4)\n",
    "        print(f'TokensEmbeding, forward time: {time.time()-t0}')\n",
    "        return all_tk_cat\n",
    "    \n",
    "    \n",
    "    def forward5(self, batch, negative_batch=None, ptime=False): # Batch_Size x SeqLen\n",
    "        # batch = batch x seq_len\n",
    "        # negative_batch = batch x seq_len x cnt_neg\n",
    "        \n",
    "        t0 = time.time()\n",
    "        rows, columns = batch.shape\n",
    "        all_tk_cat = []\n",
    "        \n",
    "        # cnt_cats - > cnt_cats x cat_emb\n",
    "        cat_emb = self.cat_emb(torch.tensor([i for i in range(self.cnt_categories)], dtype=torch.int)) # cnt_cats x cat_emb\n",
    "        \n",
    "        # cnt_cats x cat_emb -> batch x seq_len x (cnt_cats * cnt_meanings) x cat_emb\n",
    "        cat_emb_ext = einops.repeat(cat_emb, 'c e -> b s (c m) e', b=rows, s=columns, m=self.cnt_meanings) \n",
    "        \n",
    "        \n",
    "        # List (cnt_cats) of batch x seq_len x (cnt_meanings * meaning_emb)\n",
    "        all_tk_cat = [\n",
    "            emb_lay(batch.clone().apply_(getattr(self, f'cat{i}_merge').get))\n",
    "            for i, emb_lay in enumerate(self.all_tokens_embeds)\n",
    "        ]\n",
    "                \n",
    "        \n",
    "        # (List) cnt_cats x batch x seq_len x (cnt_meanings * meaning_emb) -> batch x seq_len x (cnt_cats * cnt_meanings) x meaning_emb\n",
    "        all_tk_cat = einops.rearrange(all_tk_cat, 'c b s (m e) -> b s (c m) e', m=self.cnt_meanings, e=self.meaning_emb_sz)\n",
    "        \n",
    "        # batch x seq_len x (cnt_cats * cnt_meanings) x (cat_emb + meaning_emb)\n",
    "        all_tk_cat = torch.cat([cat_emb_ext, all_tk_cat], axis=-1) \n",
    "        \n",
    "        if negative_batch is not None:\n",
    "            cnt_negative = negative_batch.shape[-1]\n",
    "            \n",
    "            negative_batch = einops.rearrange([\n",
    "                emb_lay(negative_batch.clone().apply_(getattr(self, f'cat{i}_merge').get))\n",
    "                for i, emb_lay in enumerate(self.all_tokens_embeds)], \n",
    "                'c b s n (m e) -> b s (n c m) e', \n",
    "                m=self.cnt_meanings, e=self.meaning_emb_sz\n",
    "            )\n",
    "            \n",
    "            \n",
    "            cat_emb_ext2 = einops.repeat(cat_emb, 'c e -> b s (n c m) e', b=rows, s=columns, n=cnt_negative, \n",
    "                                         m=self.cnt_meanings) \n",
    "            \n",
    "            \n",
    "            negative_batch = torch.cat([cat_emb_ext2, negative_batch], axis=-1)\n",
    "            # negative_batch = batch x seq_len x (cnt_negative * cnt_cats * cnt_meanings) x (cat_emb + meaning_emb)\n",
    "        \n",
    "        if ptime:\n",
    "            print(f'TokensEmbeding, forward5 time: {time.time()-t0}')\n",
    "        return all_tk_cat, negative_batch\n",
    "    \n",
    "    \n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)(\n",
    "            [self.cat_emb.weight, ] + [x.weight for x in self.all_tokens_embeds], lr=lr\n",
    "        )\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0708d29-00d4-4ea0-8301-9afaf198c337",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attention Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "62b11eb6-84d3-4421-b29d-ee346b203868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_sz, seed=0):\n",
    "        super(AttentionNet, self).__init__()\n",
    "        self.seed=seed\n",
    "        self.lay0 = nn.Linear(in_features=input_sz, out_features=1)\n",
    "        # self.activation = nn.LeakyReLU()\n",
    "        self.init_weights(seed)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    \n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.lay0.weight, gain=1.0)\n",
    "        self.lay0.weight.data = torch.tensor(np.array(self.lay0.weight.data), dtype=torch.float32).requires_grad_(True)\n",
    "        self.lay0.bias.data = torch.tensor(np.array(self.lay0.bias.data), dtype=torch.float32).requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch, mask):\n",
    "        # batch =  batch x seq_len x seq_len * (cnt_cats * cnt_meanings) x hidden + embs\n",
    "        seq_len = batch.shape[1]\n",
    "        cm = batch.shape[3]\n",
    "        \n",
    "        att = einops.rearrange(self.lay0(batch), 'b s_w s_h cm e -> b s_w (s_h cm e)')\n",
    "        # att =  batch x seq_len x (seq_len * cnt_cats * cnt_meanings)\n",
    "        \n",
    "        if mask is not None:\n",
    "            att = att + mask \n",
    "        # att = einops.rearrange(self.softmax(att), 'b s_w (s_h cm e) -> b s_w s_h cm e', s_h=seq_len, cm=cm, e=1)\n",
    "        att = self.softmax(att).unsqueeze(dim=-1)\n",
    "        # att = batch x seq_len x (seq_len * cnt_cats * cnt_meanings) x 1\n",
    "        return att\n",
    "\n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)([self.lay0.weight, self.lay0.bias], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced34edd-cb2a-49ed-b7e2-0cfd88fda66a",
   "metadata": {},
   "source": [
    "## Process Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "114a53a3-ccb1-4ecc-9659-524ee76a886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProceccNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, seed=0):\n",
    "        super(ProceccNet, self).__init__()\n",
    "        self.seed=seed\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.lay0 = nn.Linear(in_features=input_size, out_features=output_size)\n",
    "        self.activation = torch.tanh\n",
    "        self.init_weights(seed)\n",
    "        \n",
    "    \n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.lay0.weight, gain=1.0)\n",
    "        self.lay0.weight.data = torch.tensor(np.array(self.lay0.weight.data), dtype=torch.float32).requires_grad_(True)\n",
    "        self.lay0.bias.data = torch.tensor(np.array(self.lay0.bias.data), dtype=torch.float32).requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # batch =  Batch_Sz * SeqLen x Embs + Hidden\n",
    "        batch = self.activation(self.lay0(batch)) # batch =  Batch_Sz * SeqLen x SeqLen * Cnt_Cats * Cnt_Meanings\n",
    "        return batch\n",
    "\n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)([self.lay0.weight, self.lay0.bias], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5284f7-7575-4a8f-911a-8de8653dedd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aggregation Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "2ecd174b-c4e8-441a-b07e-0df097e2fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregationNet(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, full_embeding_size, short_embeding_size, seed=0):\n",
    "        super(AggregationNet, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.full_embeding_size = full_embeding_size\n",
    "        self.short_embeding_size = short_embeding_size\n",
    "        self.seed=seed\n",
    "        self.att_tokens = AttentionNet(self.hidden_size + self.full_embeding_size, seed=seed)\n",
    "        self.att_hidden = AttentionNet(2 * self.hidden_size, seed=seed)\n",
    "        self.process_net = ProceccNet(2 * self.hidden_size + self.short_embeding_size, self.hidden_size, seed=seed)\n",
    "        \n",
    "    def forward(self, pos_props_tokens, mask, mask_ext, rounds=10):\n",
    "        # pos_props_tokens = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x (pos_emb + cat_emb + emb_meaning)\n",
    "        \n",
    "        t0 = time.time()\n",
    "        batch_sz = pos_props_tokens.shape[0]\n",
    "        seq_len = pos_props_tokens.shape[1]\n",
    "        cm = pos_props_tokens.shape[3]\n",
    "        start_emb = self.full_embeding_size - self.short_embeding_size\n",
    "        \n",
    "        \n",
    "        # batch x seq_len x hidden\n",
    "        h = torch.zeros((batch_sz, seq_len, self.hidden_size), dtype=torch.float32).requires_grad_(False)\n",
    "        \n",
    "        all_hidens = []\n",
    "        for i in range(rounds):\n",
    "            h_ext = einops.repeat(h, 'b s_h e -> b s_w s_h cm e', s_w=seq_len, cm=cm)\n",
    "            # h_ext = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x hidden\n",
    "            \n",
    "            \n",
    "            \n",
    "            h_ext2 = einops.repeat(h, '(b s1) e -> (b s2) s1 e', b=batch_sz, s1=seq_len, s2=seq_len)\n",
    "            h_ext3 = einops.repeat(h, 'bs e -> bs s2 e', s2=seq_len)\n",
    "            \n",
    "            att_tk = self.att_tokens.forward(torch.cat([h_ext, pos_props_tokens], axis=-1), mask_ext)\n",
    "            agg_tk = (pos_props_tokens * att_tk).sum(axis=-2)\n",
    "            # att_tk = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x 1\n",
    "            \n",
    "            \n",
    "            att_h = self.att_hidden.forward(torch.cat([h_ext2, h_ext3], axis=-1), mask)\n",
    "            \n",
    "            agg_tk = (pos_props_tokens * att_tk).sum(axis=-2)\n",
    "            agg_h = (h_ext2 * att_h).sum(axis=-2)\n",
    "            \n",
    "            agg_output = torch.cat([h, agg_h, agg_tk], axis=-1)\n",
    "            h = self.process_net.forward(agg_output)\n",
    "            all_hidens.append(h)\n",
    "        print(f'AggregationNet, forward time: {time.time()-t0}')\n",
    "        return all_hidens\n",
    "\n",
    "    \n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.att_tokens.init_optims(opt_type, lr)\n",
    "        self.att_hidden.init_optims(opt_type, lr)\n",
    "        self.process_net.init_optims(opt_type, lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.att_tokens.step()\n",
    "        self.att_hidden.step()\n",
    "        self.process_net.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.att_tokens.zero_grad()\n",
    "        self.att_hidden.zero_grad()\n",
    "        self.process_net.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d59c2e-426b-4509-a238-28ab45f0ce4f",
   "metadata": {},
   "source": [
    "## Property Prediction Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "609d7c04-9ba8-4b7b-b41f-c85bafef4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PropertyNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, seed=0):\n",
    "        super(PropertyNet, self).__init__()\n",
    "        self.seed=seed\n",
    "        self.input_size = input_size\n",
    "        self.lay0 = nn.Linear(in_features=input_size, out_features=4)\n",
    "        self.activation = torch.sigmoid\n",
    "        self.init_weights(seed)\n",
    "        \n",
    "    \n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.lay0.weight, gain=1.0)\n",
    "        self.lay0.weight.data = torch.tensor(np.array(self.lay0.weight.data), dtype=torch.float32).requires_grad_(True)\n",
    "        self.lay0.bias.data = torch.tensor(np.array(self.lay0.bias.data), dtype=torch.float32).requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # batch =  Batch_Sz * SeqLen x Embs + Hidden\n",
    "        batch = self.lay0(batch) # batch =  Batch_Sz * SeqLen x SeqLen * Cnt_Cats * Cnt_Meanings\n",
    "        batch = torch.sigmoid(batch)\n",
    "        return batch\n",
    "\n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)([self.lay0.weight, self.lay0.bias], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61284703-78b3-4a1c-bce9-7da5ea844efb",
   "metadata": {},
   "source": [
    "## Category + Meaning Prediction Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "d65bfd10-b62b-4a4d-9b3e-e227a32e9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatMeangNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, seed=0):\n",
    "        super(CatMeangNet, self).__init__()\n",
    "        self.seed=seed\n",
    "        self.input_size = input_size\n",
    "        self.lay0 = nn.Linear(in_features=input_size, out_features=1)\n",
    "        self.activation = torch.sigmoid\n",
    "        self.init_weights(seed)\n",
    "        \n",
    "    \n",
    "    def init_weights(self, seed):\n",
    "        torch.manual_seed(seed)\n",
    "        nn.init.xavier_uniform_(self.lay0.weight, gain=1.0)\n",
    "        self.lay0.weight.data = torch.tensor(np.array(self.lay0.weight.data), dtype=torch.float32).requires_grad_(True)\n",
    "        self.lay0.bias.data = torch.tensor(np.array(self.lay0.bias.data), dtype=torch.float32).requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # batch =  Batch_Sz * SeqLen x Embs + Hidden\n",
    "        batch = self.lay0(batch) # batch =  Batch_Sz * SeqLen x SeqLen * Cnt_Cats * Cnt_Meanings\n",
    "        batch = torch.sigmoid(batch)\n",
    "        return batch\n",
    "\n",
    "    def init_optims(self, opt_type, lr):\n",
    "        self.current_weight_lr = lr\n",
    "        self.opt = getattr(optim, opt_type)([self.lay0.weight, self.lay0.bias], lr=lr)\n",
    "\n",
    "    def set_lr(self, lr_weight, lr_bias):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.opt.step()\n",
    "        \n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad()\n",
    "\n",
    "    def clip_grad(self, maxg=1e-2):\n",
    "        pass\n",
    "\n",
    "    def count_params(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d75ee-9050-4864-b018-c4bca069276e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "03dc580d-0cb4-435a-9f50-13e0be72a1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['First', 'First_Reverse', 'First_Second', 'First_Second_Reverse', 'Second', 'Second_Reverse'])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_VOCAB.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "ab6b477a-51a3-45c7-89fc-875019dc7f80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Token(tid=0, value='[unk]', title=False, upper=False, part=False, w_end=True)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_VOCAB['First_Second'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d55a0212-d1ed-4f4c-91b6-425373cc296b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# @lru_cache(maxsize=1000)\n",
    "def change_tokens(tkid):\n",
    "    new_tk = ALL_VOCAB['First_Second'][tkid]\n",
    "    # return new_tk.tid\n",
    "    return (new_tk.tid, int(new_tk.title), int(new_tk.upper), int(new_tk.part), int(new_tk.w_end))\n",
    "\n",
    "v_change_tokens = np.vectorize(change_tokens)\n",
    "\n",
    "def process_batch(batch):\n",
    "    return np.stack(v_change_tokens(batch), axis=2)\n",
    "\n",
    "\n",
    "def reorder_posistions(matrix, wpos):\n",
    "    sz, _ = matrix.shape\n",
    "    indx = [wpos,] + [i for i in range(sz) if i != wpos]\n",
    "    return matrix[indx,:][:,indx]\n",
    "\n",
    "def reorder_tokens(marix, w0):\n",
    "    matrix = np.array([\n",
    "        marix[i,[w0[i],] + [j for j in range(marix.shape[1]) if j != w0[i]]] \n",
    "        for i in range(marix.shape[0])\n",
    "    ])\n",
    "    return matrix\n",
    "\n",
    "def negative_tokens(tkid, cnt):\n",
    "    neg_set = set()\n",
    "    while len(neg_set) != cnt:\n",
    "        tmp_set = set(np.random.randint(0, high=len(ALL_VOCAB['Second']), size=cnt - len(neg_set), dtype=int))\n",
    "        neg_set = neg_set | tmp_set\n",
    "    return tuple(neg_set)\n",
    "\n",
    "v_negative_tokens = np.vectorize(negative_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "b21a9fb6-5a32-4eea-a5a4-10d30bc187b7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_old(nets, data, \n",
    "          batch_size, max_seq_len, cnt_negative, rounds_agg, \n",
    "          mask_coef, hidden_coef, pos_sum_coef, pad,\n",
    "          avg_info=100, opt_type='SGD', lr=1e-3, seed=0, cnt_epochs=10**6, ptime=False\n",
    "         ):\n",
    "    _ = [x.init_optims(opt_type, lr) for x in nets]\n",
    "    PosEnc, PropsEmb, TokensEmb, AggNet, PropNet, CMNet = nets\n",
    "    l2_none_loss = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    rows, _ = data.shape\n",
    "    indices = np.arange(rows)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Определяем дефолтную матрицу позиций\n",
    "    default_positions = np.array([\n",
    "        [max_seq_len + j - i - 1 for j in range(max_seq_len) ] \n",
    "        for i in range(max_seq_len)\n",
    "    ])\n",
    "    \n",
    "    hist_props_l2 = []\n",
    "    hist_props_l2_mask = []\n",
    "    \n",
    "    hist_neg_l2 = []\n",
    "    hist_neg_l2_mask = []\n",
    "    \n",
    "    hist_pos_max_l2 = []\n",
    "    hist_pos_max_l2_mask = []\n",
    "    \n",
    "    hist_pos_sum_l2 = []\n",
    "    hist_pos_sum_l2_mask = []\n",
    "\n",
    "    # Цикл по эпохам\n",
    "    for epoch_i in range(cnt_epochs):\n",
    "        np.random.shuffle(indices) # Шафлим индексы, так как шафлить массив намного дольше\n",
    "        \n",
    "        for i in range(int(rows / batch_size) + 1): # Цикл по батчам\n",
    "            batch_ids = indices[i*batch_size:(i+1)*batch_size]\n",
    "            if len(batch_ids) == 0:\n",
    "                continue # Прошлись по всей выборке\n",
    "            \n",
    "            _ = [n.zero_grad() for n in nets]\n",
    "            t0 = time.time()\n",
    "            batch = data[batch_ids,:] # Batch x SeqLen\n",
    "            batch_sz = batch.shape[0]\n",
    "            batch_len = batch.shape[1] - (batch == pad).sum(axis=1) # Опрделеям длины последовательностей в батче, (Batch_Sz,)\n",
    "            seq_len = max(batch_len)\n",
    "            if seq_len < batch.shape[1]:\n",
    "                batch = batch[:, :seq_len] # Если максимальная длина меньше дефолтной, ограничиваем массив\n",
    "                print(f'step 1_1: {batch.shape}')\n",
    "            \n",
    "            \n",
    "            batch_default_positions = default_positions[:seq_len,:seq_len] # max_len x max_len\n",
    "            center_words = list(map(np.random.randint, batch_len)) # Выбираем токен, который опустим в последовательности, (Batch_Sz,)\n",
    "            \n",
    "            batch = reorder_tokens(batch, center_words) # Меняем порядок строк/столбцов в матрице токенов, Batch_Sz x Len_Seq\n",
    "            \n",
    "            # Меняем порядок строк/столбцов в позиционной матрице, Batch_Sz x max_len x max_len\n",
    "            pos_matrix = np.stack([reorder_posistions(default_positions, x) for x in center_words])\n",
    "            \n",
    "            # Делаем замену начальных токенов на новые токены + фичи + Негативный сэмплинг\n",
    "            batch = process_batch(batch) # Batch_Sz x max_len x 5 (id, title, upper, w_end, part)\n",
    "            negative_batch = torch.tensor(einops.rearrange(\n",
    "                np.stack(v_negative_tokens(batch[:,:,0], cnt_negative), axis=2),\n",
    "                'b s n -> (b s) n'\n",
    "            ), dtype=torch.int).requires_grad_(False)\n",
    "            batch = torch.tensor(batch, dtype=torch.int).requires_grad_(False) # Batch_Sz x max_len x 5 (id, title, upper, w_end, part)\n",
    "            \n",
    "            # Получаем эмбединг позиционной матрицы, Batch_Sz x max_len x max_len x pos_emb_size\n",
    "            pos_matrix = PosEnc.forward2(torch.tensor(pos_matrix, dtype=torch.int).requires_grad_(False), ptime)\n",
    "            \n",
    "            # Получаем эмбединги фичей токенов\n",
    "            # Batch_Sz x max_len x max_len x Cnt_Cats * Cnt_Meanings x (EmbTitle + EmbUpper + EmbPart + ...)\n",
    "            props_embs = PropsEmb.forward4(batch[:,:,1:], ptime)\n",
    "            \n",
    "            # Получаем эмбединги токенов\n",
    "            tokens_embs, tokens_embs_ext, negative_batch = TokensEmb.forward5(batch[:,:,0], negative_batch, ptime)\n",
    "            \n",
    "            # tokens_embs = Batch_Sz x max_len x x max_len x Cnt_Cats * Cnt_Meanings x (CatEmb + EmbMeaning)\n",
    "            tokens_embs = einops.rearrange(tokens_embs, 'b s cm e -> (b s) cm e')\n",
    "            \n",
    "            all_tk_cat_ext = einops.repeat(all_tk_cat, 'b s_h cm e-> b s_w s_h cm e', s_w=columns)\n",
    "            \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Объединяем все матрицы в одну\n",
    "            pos_props_tokens = torch.cat([pos_matrix, props_embs, tokens_embs_ext], axis=-1)\n",
    "            \n",
    "            # Создаем маску, чтобы не учитывать центранльные токены при агрегировании\n",
    "            # TODO: Добавить логигу когда у нас будет PAD в последовательности\n",
    "            mask = torch.tensor([-np.inf,] + [1.0 for i in range(max_len - 1)], dtype=torch.float32).requires_grad_(False)\n",
    "            mask_ext = einops.repeat(mask, 's1 -> s2 (s1 m)', s2=batch_sz * max_len, \n",
    "                                 m=TokensEmb.cnt_meanings * TokensEmb.cnt_categories).requires_grad_(False)\n",
    "            \n",
    "            mask = einops.repeat(mask, 's1 -> s2 s1', s2=batch_sz * max_len, \n",
    "                                 m=TokensEmb.cnt_meanings * TokensEmb.cnt_categories).requires_grad_(False)\n",
    "            # Подаем собранные данные в агрегирующую рекурсивную сеть,\n",
    "            # На выходе массив скрытых состояний.\n",
    "            hidden_states = AggNet.forward(pos_props_tokens, mask, rounds_agg)\n",
    "            hidden_states = einops.rearrange(hidden_states, 'r bs e -> (r bs) e')\n",
    "            \n",
    "            hidden_states_ext = einops.repeat(\n",
    "                hidden_states, 'rbs e -> rbs m e',\n",
    "                m=TokensEmb.cnt_categories * TokensEmb.cnt_meanings * (cnt_negative + 1)\n",
    "            )\n",
    "            tokens_props = einops.repeat(batch[:,:,1:], 'b s f -> (m b s) f', m=rounds_agg).requires_grad_(False)\n",
    "            props_pred = PropNet.forward(hidden_states)\n",
    "            \n",
    "            loss_mask = torch.tensor([1.0, ] + [mask_coef, ] * (max_len - 1), dtype=torch.float32).requires_grad_(False)\n",
    "            loss_mask = einops.repeat(loss_mask, 's -> (rp b s) f', rp=rounds_agg, b=batch_sz, f=1).requires_grad_(False)\n",
    "            \n",
    "            \n",
    "            loss_mask2 = torch.tensor(\n",
    "                [(i/rounds_agg)**hidden_coef for i in range(1, rounds_agg + 1)],\n",
    "                dtype=torch.float32).requires_grad_(False)\n",
    "            loss_mask2 = einops.repeat(loss_mask2, 'h -> (h bs) f', bs=batch_sz*max_len , f=1).requires_grad_(False)\n",
    "            \n",
    "            props_l2 = l2_none_loss(props_pred, tokens_props.type(torch.float32)) # Лосс на свойства токенов\n",
    "            props_l2_mask = props_l2 * loss_mask * loss_mask2\n",
    "            \n",
    "            hist_props_l2.append(props_l2.mean().item())\n",
    "            hist_props_l2_mask.append(props_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_props_l2) > avg_info:\n",
    "                hist_props_l2.pop(0)\n",
    "                hist_props_l2_mask.pop(0)\n",
    "                \n",
    "                \n",
    "            # Предсказание слов\n",
    "            true_false_batch = torch.cat([tokens_embs, negative_batch], axis=1)\n",
    "            true_false_batch = einops.repeat(true_false_batch, 'bs m e -> (r bs) m e', r=rounds_agg)\n",
    "            \n",
    "            meanings_pred = CMNet.forward(torch.cat([hidden_states_ext,true_false_batch], axis=-1)).squeeze()\n",
    "            \n",
    "            # Негативный лосс\n",
    "            neg_fact = torch.zeros(\n",
    "                (rounds_agg * batch_sz * max_len, cnt_negative * TokensEmb.cnt_categories * TokensEmb.cnt_meanings),\n",
    "                dtype=torch.float32).requires_grad_(False)\n",
    "            neg_l2 = l2_none_loss(meanings_pred[:, (TokensEmb.cnt_categories * TokensEmb.cnt_meanings):], neg_fact) # Лосс на негативные значения\n",
    "            neg_l2_mask = neg_l2 * loss_mask * loss_mask2\n",
    "            \n",
    "            hist_neg_l2.append(neg_l2.mean().item())\n",
    "            hist_neg_l2_mask.append(neg_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_neg_l2) > avg_info:\n",
    "                hist_neg_l2.pop(0)\n",
    "                hist_neg_l2_mask.pop(0)\n",
    "                \n",
    "            \n",
    "            # Позитивный лосс\n",
    "            max_pred = meanings_pred[:, :(TokensEmb.cnt_categories * TokensEmb.cnt_meanings)].max(axis=1)[0]\n",
    "            pos_fact_1 = torch.ones((rounds_agg * batch_sz * max_len,), dtype=torch.float32).requires_grad_(False)\n",
    "            pos_max_l2 = l2_none_loss(max_pred, pos_fact_1)\n",
    "            pos_max_l2_mask = pos_max_l2 * loss_mask.squeeze() * loss_mask2.squeeze() \n",
    "            \n",
    "            sum_pred = meanings_pred[:, :(TokensEmb.cnt_categories * TokensEmb.cnt_meanings)].sum(axis=1)\n",
    "            pos_sum_l2 = l2_none_loss(sum_pred, pos_fact_1)\n",
    "            pos_sum_l2_mask = pos_sum_l2 * loss_mask.squeeze() * loss_mask2.squeeze() \n",
    "            \n",
    "            \n",
    "            hist_pos_max_l2.append(pos_max_l2.mean().item())\n",
    "            hist_pos_max_l2_mask.append(pos_max_l2_mask.mean().item())\n",
    "            \n",
    "            hist_pos_sum_l2.append(pos_sum_l2.mean().item())\n",
    "            hist_pos_sum_l2_mask.append(pos_sum_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_pos_max_l2) > avg_info:\n",
    "                hist_pos_max_l2.pop(0)\n",
    "                hist_pos_max_l2_mask.pop(0)\n",
    "                \n",
    "                hist_pos_sum_l2.pop(0)\n",
    "                hist_pos_sum_l2_mask.pop(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            final_loss = props_l2_mask.mean() + neg_l2_mask.mean() + pos_max_l2_mask.mean() + \\\n",
    "                pos_sum_coef * pos_sum_l2_mask.mean()\n",
    "            \n",
    "            # final_loss = props_l2_mask.mean()\n",
    "            print(f'final_loss: {final_loss.item()}')\n",
    "            final_loss.backward()\n",
    "            \n",
    "            _ = [n.step() for n in nets]\n",
    "            \n",
    "            print(f'Batch Time: {time.time() - t0}')\n",
    "            return None\n",
    "            # return (hist_props_l2[0], hist_props_l2_mask[0], hist_neg_l2[0], hist_neg_l2_mask[0], \n",
    "                    # hist_pos_max_l2[0], hist_pos_max_l2_mask[0], hist_pos_sum_l2[0], hist_pos_sum_l2_mask[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b12196d-634e-4a0e-bb84-d287105d693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(nets, data, \n",
    "          batch_size, cnt_negative, rounds_agg, \n",
    "          mask_coef, hidden_coef, pos_sum_coef, pad,\n",
    "          avg_info=100, opt_type='SGD', lr=1e-3, seed=0, max_seq_len=1024, cnt_epochs=10**6, ptime=False\n",
    "         ):\n",
    "    _ = [x.init_optims(opt_type, lr) for x in nets]\n",
    "    PosEnc, PropsEmb, TokensEmb, AggNet, PropNet, CMNet = nets\n",
    "    l2_none_loss = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    rows, _ = data.shape\n",
    "    indices = np.arange(rows)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Определяем дефолтную матрицу позиций\n",
    "    default_positions = np.array([\n",
    "        [max_seq_len + j - i - 1 for j in range(max_seq_len) ] \n",
    "        for i in range(max_seq_len)\n",
    "    ]) # Dist_ij = Dist(Token_i, Token_j)\n",
    "    \n",
    "    hist_props_l2 = []\n",
    "    hist_props_l2_mask = []\n",
    "    \n",
    "    hist_neg_l2 = []\n",
    "    hist_neg_l2_mask = []\n",
    "    \n",
    "    hist_pos_max_l2 = []\n",
    "    hist_pos_max_l2_mask = []\n",
    "    \n",
    "    hist_pos_sum_l2 = []\n",
    "    hist_pos_sum_l2_mask = []\n",
    "\n",
    "    # Цикл по эпохам\n",
    "    for epoch_i in range(cnt_epochs):\n",
    "        np.random.shuffle(indices) # Шафлим индексы, так как шафлить массив намного дольше\n",
    "        \n",
    "        for i in range(int(rows / batch_size) + 1): # Цикл по батчам\n",
    "            batch_ids = indices[i*batch_size:(i+1)*batch_size]\n",
    "            if len(batch_ids) == 0:\n",
    "                continue # Прошлись по всей выборке\n",
    "            \n",
    "            _ = [n.zero_grad() for n in nets]\n",
    "            t0 = time.time()\n",
    "            batch = data[batch_ids,:] # Batch x SeqLen\n",
    "            batch_sz = batch.shape[0]\n",
    "            batch_len = batch.shape[1] - (batch == pad).sum(axis=1) # Опрделеям длины последовательностей в батче, (Batch,)\n",
    "            seq_len = max(batch_len)\n",
    "            if seq_len < batch.shape[1]:\n",
    "                batch = batch[:, :seq_len] # Если максимальная длина меньше дефолтной, ограничиваем массив\n",
    "                print(f'step 1_1: {batch.shape}')\n",
    "            \n",
    "            \n",
    "            pos_matrix = default_positions[:seq_len,:seq_len] # seq_len x seq_len\n",
    "            \n",
    "            # Делаем замену начальных токенов на новые токены + фичи + Негативный сэмплинг\n",
    "            batch = process_batch(batch) # batch x seq_len x 5 (id, title, upper, w_end, part)\n",
    "            batch = torch.tensor(batch, dtype=torch.int).requires_grad_(False) # batch x seq_len x 5 (id, title, upper, w_end, part)\n",
    "            \n",
    "            negative_batch = torch.tensor(\n",
    "                np.stack(v_negative_tokens(batch[:,:,0], cnt_negative), axis=2), \n",
    "                dtype=torch.int\n",
    "            ).requires_grad_(False) # batch x seq_len x cnt_negative\n",
    "            \n",
    "            \n",
    "            # Получаем эмбединг позиционной матрицы, seq_len x seq_len x pos_emb\n",
    "            pos_matrix = PosEnc.forward2(torch.tensor(pos_matrix, dtype=torch.int).requires_grad_(False), ptime)\n",
    "            pos_matrix = einops.repeat('s_w s_h e -> b s_w s_h (c m) e', b=batch_sz, \n",
    "                                       c=TokensEmb.cnt_categories, m=TokensEmb.cnt_meanings)\n",
    "            # pos_matrix = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x pos_emb\n",
    "            \n",
    "            \n",
    "            # Получаем эмбединги фичей токенов\n",
    "            props_embs = PropsEmb.forward4(batch[:,:,1:], ptime)\n",
    "            # batch x seq_len x props_embs\n",
    "            \n",
    "            props_embs = einops.repeat(props_embs, 'b s_h e -> b s_w s_h cm e', s_w=seq_len, cm=self.cnt_repeats) \n",
    "            # props_embs = batch x seq_len (s_w) x seq_len (s_h) x (cnt_cats * cnt_meanings) x props_embs\n",
    "            \n",
    "            # Получаем эмбединги токенов\n",
    "            tokens_embs, negative_batch = TokensEmb.forward5(batch[:,:,0], negative_batch, ptime)\n",
    "            # tokens_embs = batch x seq_len x (cnt_cats * cnt_meanings) x (cat_emb + emb_meaning)\n",
    "            # negative_batch = batch x seq_len x (cnt_negative * cnt_cats * cnt_meanings) x (cat_emb + meaning_emb)\n",
    "\n",
    "            # tokens_embs_ext = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x (cat_emb + emb_meaning)\n",
    "            tokens_embs_ext = einops.repeat(tokens_embs, 'b s_h cm e-> b s_w s_h cm e', s_w=seq_len)\n",
    "            \n",
    "            \n",
    "            tokens_embs = einops.rearrange(tokens_embs, 'b s cm e -> (b s) cm e')\n",
    "            # tokens_embs = (batch * seq_len) x (cnt_cats * cnt_meanings) x (cat_emb + emb_meaning)\n",
    "            \n",
    "            \n",
    "            # Объединяем все матрицы в одну\n",
    "            pos_props_tokens = torch.cat([pos_matrix, props_embs, tokens_embs_ext], axis=-1)\n",
    "            # pos_props_tokens = batch x seq_len x seq_len x (cnt_cats * cnt_meanings) x (pos_emb + cat_emb + emb_meaning)\n",
    "            \n",
    "            # TODO: Добавить логику маскирования pad токенов.\n",
    "            # Подаем собранные данные в агрегирующую рекурсивную сеть,\n",
    "            # На выходе массив скрытых состояний.\n",
    "            hidden_states = AggNet.forward(pos_props_tokens, None, rounds_agg)\n",
    "            hidden_states = einops.rearrange(hidden_states, 'r bs e -> (r bs) e')\n",
    "            \n",
    "            hidden_states_ext = einops.repeat(\n",
    "                hidden_states, 'rbs e -> rbs m e',\n",
    "                m=TokensEmb.cnt_categories * TokensEmb.cnt_meanings * (cnt_negative + 1)\n",
    "            )\n",
    "            tokens_props = einops.repeat(batch[:,:,1:], 'b s f -> (m b s) f', m=rounds_agg).requires_grad_(False)\n",
    "            props_pred = PropNet.forward(hidden_states)\n",
    "            \n",
    "            loss_mask = torch.tensor([1.0, ] + [mask_coef, ] * (max_len - 1), dtype=torch.float32).requires_grad_(False)\n",
    "            loss_mask = einops.repeat(loss_mask, 's -> (rp b s) f', rp=rounds_agg, b=batch_sz, f=1).requires_grad_(False)\n",
    "            \n",
    "            \n",
    "            loss_mask2 = torch.tensor(\n",
    "                [(i/rounds_agg)**hidden_coef for i in range(1, rounds_agg + 1)],\n",
    "                dtype=torch.float32).requires_grad_(False)\n",
    "            loss_mask2 = einops.repeat(loss_mask2, 'h -> (h bs) f', bs=batch_sz*max_len , f=1).requires_grad_(False)\n",
    "            \n",
    "            props_l2 = l2_none_loss(props_pred, tokens_props.type(torch.float32)) # Лосс на свойства токенов\n",
    "            props_l2_mask = props_l2 * loss_mask * loss_mask2\n",
    "            \n",
    "            hist_props_l2.append(props_l2.mean().item())\n",
    "            hist_props_l2_mask.append(props_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_props_l2) > avg_info:\n",
    "                hist_props_l2.pop(0)\n",
    "                hist_props_l2_mask.pop(0)\n",
    "                \n",
    "                \n",
    "            # Предсказание слов\n",
    "            true_false_batch = torch.cat([tokens_embs, negative_batch], axis=1)\n",
    "            true_false_batch = einops.repeat(true_false_batch, 'bs m e -> (r bs) m e', r=rounds_agg)\n",
    "            \n",
    "            meanings_pred = CMNet.forward(torch.cat([hidden_states_ext,true_false_batch], axis=-1)).squeeze()\n",
    "            \n",
    "            # Негативный лосс\n",
    "            neg_fact = torch.zeros(\n",
    "                (rounds_agg * batch_sz * max_len, cnt_negative * TokensEmb.cnt_categories * TokensEmb.cnt_meanings),\n",
    "                dtype=torch.float32).requires_grad_(False)\n",
    "            neg_l2 = l2_none_loss(meanings_pred[:, (TokensEmb.cnt_categories * TokensEmb.cnt_meanings):], neg_fact) # Лосс на негативные значения\n",
    "            neg_l2_mask = neg_l2 * loss_mask * loss_mask2\n",
    "            \n",
    "            hist_neg_l2.append(neg_l2.mean().item())\n",
    "            hist_neg_l2_mask.append(neg_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_neg_l2) > avg_info:\n",
    "                hist_neg_l2.pop(0)\n",
    "                hist_neg_l2_mask.pop(0)\n",
    "                \n",
    "            \n",
    "            # Позитивный лосс\n",
    "            max_pred = meanings_pred[:, :(TokensEmb.cnt_categories * TokensEmb.cnt_meanings)].max(axis=1)[0]\n",
    "            pos_fact_1 = torch.ones((rounds_agg * batch_sz * max_len,), dtype=torch.float32).requires_grad_(False)\n",
    "            pos_max_l2 = l2_none_loss(max_pred, pos_fact_1)\n",
    "            pos_max_l2_mask = pos_max_l2 * loss_mask.squeeze() * loss_mask2.squeeze() \n",
    "            \n",
    "            sum_pred = meanings_pred[:, :(TokensEmb.cnt_categories * TokensEmb.cnt_meanings)].sum(axis=1)\n",
    "            pos_sum_l2 = l2_none_loss(sum_pred, pos_fact_1)\n",
    "            pos_sum_l2_mask = pos_sum_l2 * loss_mask.squeeze() * loss_mask2.squeeze() \n",
    "            \n",
    "            \n",
    "            hist_pos_max_l2.append(pos_max_l2.mean().item())\n",
    "            hist_pos_max_l2_mask.append(pos_max_l2_mask.mean().item())\n",
    "            \n",
    "            hist_pos_sum_l2.append(pos_sum_l2.mean().item())\n",
    "            hist_pos_sum_l2_mask.append(pos_sum_l2_mask.mean().item())\n",
    "            \n",
    "            if len(hist_pos_max_l2) > avg_info:\n",
    "                hist_pos_max_l2.pop(0)\n",
    "                hist_pos_max_l2_mask.pop(0)\n",
    "                \n",
    "                hist_pos_sum_l2.pop(0)\n",
    "                hist_pos_sum_l2_mask.pop(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "            final_loss = props_l2_mask.mean() + neg_l2_mask.mean() + pos_max_l2_mask.mean() + \\\n",
    "                pos_sum_coef * pos_sum_l2_mask.mean()\n",
    "            \n",
    "            # final_loss = props_l2_mask.mean()\n",
    "            print(f'final_loss: {final_loss.item()}')\n",
    "            final_loss.backward()\n",
    "            \n",
    "            _ = [n.step() for n in nets]\n",
    "            \n",
    "            print(f'Batch Time: {time.time() - t0}')\n",
    "            return None\n",
    "            # return (hist_props_l2[0], hist_props_l2_mask[0], hist_neg_l2[0], hist_neg_l2_mask[0], \n",
    "                    # hist_pos_max_l2[0], hist_pos_max_l2_mask[0], hist_pos_sum_l2[0], hist_pos_sum_l2_mask[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "d7859eb8-e046-496e-968e-b07153c64ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PosEnc = PosEncoding(\n",
    "    rows=(MAX_SEQ_LEN - 1) * 2 + 1, \n",
    "    emb_size=POS_EMB_SIZE,\n",
    "    cnt_repeats=CNT_MEANINGS * len(CAT_SIZES)\n",
    ")\n",
    "PropsEmb = PropsEmbeding(\n",
    "    title=TITLE_EMB_SIZE, \n",
    "    upper=UPPER_EMB_SIZE, \n",
    "    part=PART_EMB_SIZE, \n",
    "    w_end=END_EMB_SIZE,\n",
    "    cnt_repeats=CNT_MEANINGS * len(CAT_SIZES),\n",
    ")\n",
    "\n",
    "TokensEmb = TokensEmbeding(\n",
    "    cnt_tokens=len(ALL_VOCAB['Second']), \n",
    "    categories_sz=CAT_SIZES, \n",
    "    cnt_meanings=CNT_MEANINGS,\n",
    "    meaning_emb_sz=MEANING_EMB_SIZE,\n",
    "    cat_emb_sz=CAT_EMB_SIZE\n",
    ")\n",
    "\n",
    "AggNet = AggregationNet(\n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    full_embeding_size=INPUT_SIZE,\n",
    "    short_embeding_size=PREDICT_SIZE\n",
    ")\n",
    "\n",
    "PropNet = PropertyNet(\n",
    "    input_size=HIDDEN_SIZE\n",
    ")\n",
    "\n",
    "CMNet = CatMeangNet(\n",
    "    input_size=HIDDEN_SIZE+CAT_EMB_SIZE+MEANING_EMB_SIZE\n",
    ")\n",
    "\n",
    "\n",
    "MAX_POSITION_MATRIX = np.array([\n",
    "        [MAX_SEQ_LEN + j - i - 1 for j in range(MAX_SEQ_LEN)] \n",
    "        for i in range(MAX_SEQ_LEN)\n",
    "    ]) # Dist_ij = Dist(Token_i, Token_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a4fff7f3-017a-41ec-8c6f-a9e115351212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_POSITION_MATRIX.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f69493-8dee-4a4d-ab65-5a6ebf643430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1279fd51-3cd3-43ae-b171-cdff17dc61bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "0f06c6b9-579b-44ee-8f04-1eec725da422",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([480, 30, 150]) torch.Size([480, 600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (30) must match the size of tensor b (600) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [344]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m returned_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mPosEnc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPropsEmb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTokensEmb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAggNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPropNet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCMNet\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_SEQ_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcnt_negative\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCNT_NEGATIVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrounds_agg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAGG_ROUNDS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopt_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSGD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpos_sum_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrapped_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mptime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [342]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(nets, data, batch_size, max_seq_len, cnt_negative, rounds_agg, mask_coef, hidden_coef, pos_sum_coef, pad, avg_info, opt_type, lr, seed, cnt_epochs, ptime)\u001b[0m\n\u001b[1;32m     85\u001b[0m mask \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrepeat(mask, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms1 -> s2 (s1 m)\u001b[39m\u001b[38;5;124m'\u001b[39m, s2\u001b[38;5;241m=\u001b[39mbatch_sz \u001b[38;5;241m*\u001b[39m max_len, \n\u001b[1;32m     86\u001b[0m                      m\u001b[38;5;241m=\u001b[39mTokensEmb\u001b[38;5;241m.\u001b[39mcnt_meanings \u001b[38;5;241m*\u001b[39m TokensEmb\u001b[38;5;241m.\u001b[39mcnt_categories)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Подаем собранные данные в агрегирующую рекурсивную сеть,\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# На выходе массив скрытых состояний.\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mAggNet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_props_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrounds_agg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(hidden_states, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr bs e -> (r bs) e\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m hidden_states_ext \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrepeat(\n\u001b[1;32m     93\u001b[0m     hidden_states, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbs e -> rbs m e\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     94\u001b[0m     m\u001b[38;5;241m=\u001b[39mTokensEmb\u001b[38;5;241m.\u001b[39mcnt_categories \u001b[38;5;241m*\u001b[39m TokensEmb\u001b[38;5;241m.\u001b[39mcnt_meanings \u001b[38;5;241m*\u001b[39m (cnt_negative \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     95\u001b[0m )\n",
      "Input \u001b[0;32mIn [336]\u001b[0m, in \u001b[0;36mAggregationNet.forward\u001b[0;34m(self, pos_props_tokens, mask, rounds)\u001b[0m\n\u001b[1;32m     33\u001b[0m att_tk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matt_tokens\u001b[38;5;241m.\u001b[39mforward(torch\u001b[38;5;241m.\u001b[39mcat([h_ext, pos_props_tokens], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), mask)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcat([h_ext2, h_ext3], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape, mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 36\u001b[0m att_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt_hidden\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mh_ext2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_ext3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m agg_tk \u001b[38;5;241m=\u001b[39m (props_tokens \u001b[38;5;241m*\u001b[39m att_tk)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     39\u001b[0m agg_h \u001b[38;5;241m=\u001b[39m (h_ext2 \u001b[38;5;241m*\u001b[39m att_h)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "Input \u001b[0;32mIn [252]\u001b[0m, in \u001b[0;36mAttentionNet.forward\u001b[0;34m(self, batch, mask)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, mask):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# batch =  Batch_Sz * SeqLen x SeqLen * Cnt_Cats * Cnt_Meanings x Embs + Hidden\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlay0(batch)\u001b[38;5;241m.\u001b[39msqueeze()  \u001b[38;5;66;03m# batch =  Batch_Sz * SeqLen x SeqLen * Cnt_Cats * Cnt_Meanings\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     att \u001b[38;5;241m=\u001b[39m \u001b[43matt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m \n\u001b[1;32m     23\u001b[0m     att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(att)\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m att\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (30) must match the size of tensor b (600) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "returned_vals = train(\n",
    "    nets=[PosEnc, PropsEmb, TokensEmb, AggNet, PropNet, CMNet],\n",
    "    data=train_tokens, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    cnt_negative=CNT_NEGATIVE,\n",
    "    rounds_agg=AGG_ROUNDS,\n",
    "    opt_type='SGD', \n",
    "    lr=1e-3, \n",
    "    mask_coef=0.8,\n",
    "    hidden_coef=1.15,\n",
    "    pos_sum_coef=0.05,\n",
    "    pad=wrapped_tokenizer.pad_token_id,\n",
    "    ptime=False\n",
    ")\n",
    "# max_pred, pos_fact_1, pos_max_l2 = returned_vals\n",
    "# hist_props_l2, hist_props_l2_mask, hist_neg_l2, hist_neg_l2_mask, hist_pos_max_l2, hist_pos_max_l2_mask, hist_pos_sum_l2, hist_pos_sum_l2_mask = returned_vals\n",
    "# props_embs = [Batch, SeqLen, SeqLen, CntCats * CntMeanings, PropsEmbs]\n",
    "# props_embs[Any, i, j, Any, :] == props_embs[Any, t, j, Any, :] \n",
    "# props_embs[Any, i, j, Any, :] != props_embs[Any, i, r, Any, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "a47a9054-3ac0-4855-9911-86e682458570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([\n",
    "        [1024 + j - i - 1 for j in range(1024) ] \n",
    "        for i in range(1024)\n",
    "    ]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "e29cca73-76ff-4d73-bfcf-6c1dd24ea17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_fact.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e3d0cc06-ec60-4f8e-b335-e714e0affc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.int32)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_fact.type(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "cf32cd82-80e9-4858-ba62-72bb69795a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[29, 25, 26, ..., 52, 53, 54],\n",
       "        [33, 29, 30, ..., 56, 57, 58],\n",
       "        [32, 28, 29, ..., 55, 56, 57],\n",
       "        ...,\n",
       "        [ 6,  2,  3, ..., 29, 30, 31],\n",
       "        [ 5,  1,  2, ..., 28, 29, 30],\n",
       "        [ 4,  0,  1, ..., 27, 28, 29]],\n",
       "\n",
       "       [[29, 21, 22, ..., 48, 49, 50],\n",
       "        [37, 29, 30, ..., 56, 57, 58],\n",
       "        [36, 28, 29, ..., 55, 56, 57],\n",
       "        ...,\n",
       "        [10,  2,  3, ..., 29, 30, 31],\n",
       "        [ 9,  1,  2, ..., 28, 29, 30],\n",
       "        [ 8,  0,  1, ..., 27, 28, 29]],\n",
       "\n",
       "       [[29,  1,  2, ..., 27, 28, 30],\n",
       "        [57, 29, 30, ..., 55, 56, 58],\n",
       "        [56, 28, 29, ..., 54, 55, 57],\n",
       "        ...,\n",
       "        [31,  3,  4, ..., 29, 30, 32],\n",
       "        [30,  2,  3, ..., 28, 29, 31],\n",
       "        [28,  0,  1, ..., 26, 27, 29]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[29, 19, 20, ..., 46, 47, 48],\n",
       "        [39, 29, 30, ..., 56, 57, 58],\n",
       "        [38, 28, 29, ..., 55, 56, 57],\n",
       "        ...,\n",
       "        [12,  2,  3, ..., 29, 30, 31],\n",
       "        [11,  1,  2, ..., 28, 29, 30],\n",
       "        [10,  0,  1, ..., 27, 28, 29]],\n",
       "\n",
       "       [[29, 13, 14, ..., 40, 41, 42],\n",
       "        [45, 29, 30, ..., 56, 57, 58],\n",
       "        [44, 28, 29, ..., 55, 56, 57],\n",
       "        ...,\n",
       "        [18,  2,  3, ..., 29, 30, 31],\n",
       "        [17,  1,  2, ..., 28, 29, 30],\n",
       "        [16,  0,  1, ..., 27, 28, 29]],\n",
       "\n",
       "       [[29, 19, 20, ..., 46, 47, 48],\n",
       "        [39, 29, 30, ..., 56, 57, 58],\n",
       "        [38, 28, 29, ..., 55, 56, 57],\n",
       "        ...,\n",
       "        [12,  2,  3, ..., 29, 30, 31],\n",
       "        [11,  1,  2, ..., 28, 29, 30],\n",
       "        [10,  0,  1, ..., 27, 28, 29]]])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returned_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c6126097-8365-43df-b457-42a4c97b859a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.25330349802970886, 0.01613856665790081)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_props_l2, hist_props_l2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2f1778cb-bcfe-4ce2-8618-16f459a68248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.21183332800865173, 0.01289786770939827)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_neg_l2, hist_neg_l2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "411ffbd5-ceb5-4ea9-8ba3-0cc65fb6c1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26231759786605835, 0.019070377573370934)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_pos_max_l2, hist_pos_max_l2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "de851be3-c786-4101-b08a-4118fb136853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66.89488220214844, 4.0206427574157715)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_pos_sum_l2, hist_pos_sum_l2_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95a73033-2a88-428b-870c-f7167b1b334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_fact = torch.zeros((10 * 480, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a42b55fb-dd70-4510-8d28-ecd8029a917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_none_loss = torch.nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1194204-1aa6-41ef-987c-bd8a30580552",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_pred = l2_none_loss(neg_fact, meanings_pred[:, 20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e649322-1d6f-4926-8ac5-aa3bb90253f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 100])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b6dfcfa-1445-411d-96ff-b15d247184f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 100])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(neg_pred * loss_mask2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73970107-bb0e-4e79-b11c-29102fcf3cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aedd3e37-23bb-4ff5-a57c-04502436525a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1571])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2[480,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "06abdf7c-824d-4cc9-8889-82d704c66e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 120, 180])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states_ext.shape\n",
    "# hidden_states = einops.repeat(hidden_states, 'r bs e -> (r bs) m e',\n",
    "# m=TokensEmb.cnt_categories * TokensEmb.cnt_meanings * cnt_negative, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4ec86a-82ed-41a2-bc61-a9d27e161637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [4800, 120, 180] = [10(agg) x 16 (bs) 30(seq), 120, 180] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "6f366a90-cf12-47c4-9d6e-a8f542dfaa97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 120, 80])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_false_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "58ebd46f-7ae5-45b4-aec9-0906b2373b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 120, 80])"
      ]
     },
     "execution_count": 705,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_false_batch = torch.cat([tokens_embs, negative_batch], axis=1)\n",
    "true_false_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "e7cf3326-ce66-4656-ad70-7ef69de20a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([480, 100, 30]), torch.Size([480, 100, 50]))"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([cat_emb_ext2, negative_batch2], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "38cdfc05-cb29-408d-802e-9fbe2e52763b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1312, -0.0948,  0.2202,  0.1414,  0.2555,  0.4131,  0.3525, -0.3640,\n",
       "        -0.0362,  0.1579, -0.0383, -0.3065,  0.1020, -0.2766, -0.3648,  0.1268,\n",
       "         0.3612,  0.1698,  0.0396, -0.1943, -0.0156,  0.1032, -0.1395,  0.2881,\n",
       "         0.1752, -0.4159, -0.2856, -0.0416, -0.0387,  0.4081],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_emb_ext2[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "id": "ba18b4d3-e184-4399-af8b-6072a27fd88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0022,  0.0484,  0.1391,  0.3842, -0.0386,  0.2287,  0.2664, -0.2704,\n",
       "        -0.2611,  0.2052,  0.1788,  0.4159,  0.0611, -0.3091, -0.4118,  0.2832,\n",
       "        -0.1521, -0.4052, -0.1839,  0.3027, -0.3857, -0.3729,  0.0468, -0.3128,\n",
       "         0.3849,  0.0253,  0.0616, -0.1326, -0.2529, -0.1535],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 691,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_emb_ext2[0,25,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "5bd4f027-fdb4-46dc-951e-abf1610f2a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 25, 50])"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "04344c02-dbe2-4aea-a57c-4e1fb205449f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 4])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_props.shape)\n",
    "tokens_props = einops.repeat(tokens_props, 'b s f -> (m b s) f', m=10)\n",
    "tokens_props.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "bbd67e2e-e26e-42a3-9107-7da376805ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 180])"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "3ea5c383-27df-4a1e-b0b2-7933bca65655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 180])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states2 = einops.rearrange(hidden_states, 'r bs e -> (r bs) e')\n",
    "hidden_states2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "642e6809-81e6-4157-8a19-07bd64c185fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = PropNet.forward(hidden_states2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "705f90cc-928f-4f1a-a150-809a20f522f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 4])"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_props.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "643b8fed-6d1f-4430-a3c4-6b03b56b77f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 4])"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "946b899b-b5c1-4edf-a591-7b4a12d7a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_mean_loss = torch.nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "9d3497d5-0edd-4f3e-989a-8f9edcd485f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 4])"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_mean_loss(tmp, tokens_props).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "ebe17d95-35c3-4adc-abdb-6e2024735ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mask = torch.tensor([1.0, ] + [0.05] * 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "69a71c77-fe17-4b94-abef-89407b0c013f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0708, 0.1571, 0.2504, 0.3486, 0.4506, 0.5557, 0.6635, 0.7737, 0.8859,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2 = torch.tensor([(i/10)**1.15 for i in range(1,11)])\n",
    "loss_mask2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "720a8153-3b4a-4566-82c4-6fbef889d412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0794],\n",
       "         [0.0794],\n",
       "         [0.0794],\n",
       "         ...,\n",
       "         [1.0000],\n",
       "         [1.0000],\n",
       "         [1.0000]]),\n",
       " torch.Size([4800, 1]))"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2 = torch.tensor([(i/10)**1.1 for i in range(1,11)])\n",
    "loss_mask2 =einops.repeat(loss_mask2, 'h -> (h bs) f', bs=480, f=1)\n",
    "loss_mask2, loss_mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "dc8cbcb5-cc21-467a-8b6a-0d0d2ea2eab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 1])"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2 =einops.repeat(loss_mask2, 'h -> (h bs) f', bs=480, f=1)\n",
    "loss_mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "161f47ed-41e5-4eac-af0e-e83185bd5ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0631])"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2[479,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "1d2b2b66-e3e1-4fec-9b1d-d501aecb724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4800, 1])"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2 = einops.repeat(loss_mask, 's -> (rp b s) f', rp=10, b=16, f=1)\n",
    "loss_mask2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "ec969f70-8e58-493c-a984-b02bf826ba2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "34a4ad90-6eb0-4b60-8e7c-43e1b6aa9f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_mask2[30,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f268d0-5ed9-4989-a3b5-7f40c01784c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638bcb16-53e4-4ac6-a41b-a734a7e9e5fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba37041c-6066-41f7-961a-f7ca7db32a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "811c8006-6002-482b-8859-a553f45abbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1631, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_mean_loss(tmp, tokens_props).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20257ae6-0787-4ab5-a524-9cb8d3077a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "5e2037b7-1b60-48cb-a8a6-9dd4d42d2694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  cnt\n",
       "0  0  0  0  0    6\n",
       "1  0  0  0  1  394\n",
       "2  0  0  1  0    5\n",
       "3  0  0  1  1   19\n",
       "4  1  0  0  0   12\n",
       "5  1  0  0  1   40\n",
       "6  1  1  0  0    1\n",
       "7  1  1  0  1    3"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tokens_props).groupby([0,1,2,3]).size().to_frame('cnt').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "52416c08-92cf-48a2-8063-bcb42f6259ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 230 ms, sys: 177 ms, total: 408 ms\n",
      "Wall time: 84.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ftmp = torch.cat([pos_matrix, props_embs, tokens_embs], axis=4)\n",
    "ftmp = einops.rearrange(ftmp, 'b s1 s2 m e -> (b s1) (s2 m) e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "bae97352-07a0-49c4-92ef-349a479b72c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 433 ms, sys: 454 ms, total: 887 ms\n",
      "Wall time: 194 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tmp1 = einops.rearrange(pos_matrix, 'b s1 s2 m e -> (b s1) (s2 m) e')\n",
    "tmp2 = einops.rearrange(props_embs, 'b s1 s2 m e -> (b s1) (s2 m) e')\n",
    "tmp3 = einops.rearrange(tokens_embs, 'b s1 s2 m e -> (b s1) (s2 m) e')\n",
    "ftmp = torch.cat([tmp1, tmp2, tmp3], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "fcc4864d-7cac-4f94-a554-eccb21530601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 287 ms, sys: 203 ms, total: 491 ms\n",
      "Wall time: 127 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ftmp = torch.cat([tmp1, tmp2, tmp3], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "be44a014-4b01-4c08-8136-8dc24a290913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 600, 140])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1e3817dd-9138-4d4d-917b-d3ada234ee3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.0"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "140 * 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "fd3a2cbb-2065-4c00-bfc5-7f03397e24e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 30, 20, 20])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "f374f15d-e59f-46b6-b601-af169019d90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 30, 20, 40])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "ae5ffed9-17aa-4363-97bb-7ce28fc85468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 30, 20, 80])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "03c67b3c-ac62-49f9-a897-f91f6873d3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_EMB_SIZE + TITLE_EMB_SIZE + UPPER_EMB_SIZE + PART_EMB_SIZE + END_EMB_SIZE + CAT_EMB_SIZE + MEANING_EMB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "4978174f-6d75-41f8-a7e5-5ff51facc2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TITLE_EMB_SIZE + UPPER_EMB_SIZE + PART_EMB_SIZE + END_EMB_SIZE + CAT_EMB_SIZE + MEANING_EMB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3ed87fd2-802d-41c4-91e1-babb5fe4b31b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [10, 10] at entry 0 and [10, 20] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [261]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43meinops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrearrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ml a b -> a (l b)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Wiki_103/venv/lib/python3.9/site-packages/einops/einops.py:486\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRearrange can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be applied to an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 486\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_on_zeroth_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrearrange\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths)\n",
      "File \u001b[0;32m~/Documents/Wiki_103/venv/lib/python3.9/site-packages/einops/_backends.py:334\u001b[0m, in \u001b[0;36mTorchBackend.stack_on_zeroth_dimension\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_on_zeroth_dimension\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors: \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [10, 10] at entry 0 and [10, 20] at entry 1"
     ]
    }
   ],
   "source": [
    "_ = einops.rearrange([a, b, c], 'l a b -> a (l b)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "123515b1-1463-4654-8265-99e9a4e4cc57",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [128, 30, 30, 20, 20] at entry 0 and [128, 30, 30, 20, 40] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Wiki_103/venv/lib/python3.9/site-packages/einops/einops.py:486\u001b[0m, in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRearrange can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be applied to an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 486\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack_on_zeroth_dimension\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(tensor, pattern, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrearrange\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39maxes_lengths)\n",
      "File \u001b[0;32m~/Documents/Wiki_103/venv/lib/python3.9/site-packages/einops/_backends.py:334\u001b[0m, in \u001b[0;36mTorchBackend.stack_on_zeroth_dimension\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstack_on_zeroth_dimension\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensors: \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [128, 30, 30, 20, 20] at entry 0 and [128, 30, 30, 20, 40] at entry 1"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_ = einops.rearrange([pos_matrix, props_embs, tokens_embs],\n",
    "                     'l b s1 s2 c e -> b s1 s2 c e'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d37c02-f475-4c47-9609-ce4f30c51980",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5bf9eb12-82ce-4578-a69a-31859dc57b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0785, -0.2282, -0.2500,  0.0691, -0.0209, -0.1392,  0.0557,  0.1047,\n",
       "         0.2192,  0.2140, -0.0413, -0.2430, -0.2490,  0.2573,  0.1218,  0.1201,\n",
       "        -0.2384,  0.2552,  0.2611,  0.2488], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix[0,0,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "830c8427-1623-4d30-9e6d-8b7b2cd2ca3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0785, -0.2282, -0.2500,  0.0691, -0.0209, -0.1392,  0.0557,  0.1047,\n",
       "         0.2192,  0.2140, -0.0413, -0.2430, -0.2490,  0.2573,  0.1218,  0.1201,\n",
       "        -0.2384,  0.2552,  0.2611,  0.2488], dtype=torch.float64,\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix[0,0,0,15,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a412a279-7381-4a16-9414-a4aaf49b7fb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0691, -0.0598,  0.2026, -0.2136,  0.0293,  0.2592, -0.0379,  0.2140,\n",
       "        -0.0849,  0.2218, -0.2666, -0.0397, -0.0484,  0.0893,  0.1081,  0.2116,\n",
       "        -0.0411, -0.0109,  0.1887, -0.0746], dtype=torch.float64,\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_matrix[0,1,0,5,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "b03fe569-7e51-4333-b375-8e76d4a7441a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "        True, True, True, True])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs[0,0,0,0,:] == props_embs[0,0,2,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49338fc-27cd-49df-a6ae-b143cb0884b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "props_embs[:,0,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "694a19a1-ece3-4691-bdfb-8a05a563b152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokensEmbeding, forward time: 0.4157431125640869\n",
      "CPU times: user 580 ms, sys: 868 ms, total: 1.45 s\n",
      "Wall time: 417 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var = TokensEmb.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e20ef071-e904-4c78-8b10-29be044735eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokensEmbeding, forward2 time: 3.1199347972869873\n",
      "CPU times: user 2.94 s, sys: 4.99 s, total: 7.93 s\n",
      "Wall time: 3.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var2 = TokensEmb.forward2(extend_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5963b1d-9881-4de8-ad54-de1ca8ed5766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokensEmbeding, forward3 time: 0.0433809757232666\n",
      "CPU times: user 79.1 ms, sys: 115 ms, total: 194 ms\n",
      "Wall time: 44.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var3 = TokensEmb.forward3(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abbb1c95-a5d6-4c07-9633-4291090e02ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokensEmbeding, forward3 time: 0.032254934310913086\n",
      "CPU times: user 73.2 ms, sys: 55.6 ms, total: 129 ms\n",
      "Wall time: 33.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var4 = TokensEmb.forward4(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ceb17729-2692-4810-b29e-1580219a8cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_emb: torch.Size([128, 30, 20, 30])\n",
      "cnt_categories: 4\n",
      "cnt_meanings: 5\n",
      "cat_emb: torch.Size([128, 30, 20, 50])\n",
      "TokensEmbeding, forward5 time: 0.03566884994506836\n",
      "CPU times: user 67.5 ms, sys: 90.3 ms, total: 158 ms\n",
      "Wall time: 37.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var5 = TokensEmb.forward5(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac8125-c1cf-4ca2-831d-8e766a24127e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abab7865-2750-4a69-9a18-9d147afc40cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(184320000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(old_var == old_var4).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "f29f9d5e-3105-41c5-a271-e8f1dada74e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30, 30, 20, 80])"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_var3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "22c860cc-04a2-44d7-80fc-cb8d0a77492f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184320000"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128 * 30 * 30 * 20 * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e77996-408e-4b9c-ae9a-f9ab87aae485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c72a5e0-1eb8-4075-b299-7693beb510b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9146d95-f0b4-49da-babc-7ff4fa4118a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37552c-815c-4f1b-ba27-0e52da73e9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "id": "1fd64c6f-794d-448c-ba8c-3059d6a78f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropsEmbeding, forward time: 0.24170589447021484\n",
      "CPU times: user 350 ms, sys: 489 ms, total: 839 ms\n",
      "Wall time: 262 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var = PropsEmb.forward(extend_batch[:,:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "id": "87bfdb0a-e134-44bb-bbb5-25e7d0fb3abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropsEmbeding, forward2 time: 0.6497049331665039\n",
      "CPU times: user 1.25 s, sys: 1.01 s, total: 2.26 s\n",
      "Wall time: 650 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var2 = PropsEmb.forward2(extend_batch[:,:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "7fa92d08-c5c1-4c29-99f0-4277e64d9efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropsEmbeding, forward3 time: 0.20324993133544922\n",
      "CPU times: user 224 ms, sys: 410 ms, total: 634 ms\n",
      "Wall time: 203 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var3 = PropsEmb.forward3(batch[:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "957518c2-4565-4155-be62-bfb50cf9aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PropsEmbeding, forward4 time: 0.0036687850952148438\n",
      "CPU times: user 2.26 ms, sys: 2.88 ms, total: 5.14 ms\n",
      "Wall time: 3.81 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_var = PropsEmb.forward4(batch[:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "974b86cc-66ad-494a-8e24-d149bc57e657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosEncoding, forward time: 0.06662583351135254\n",
      "CPU times: user 128 ms, sys: 222 ms, total: 350 ms\n",
      "Wall time: 69.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "old_var = PosEnc.forward(torch.tensor(pos_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "455ff992-d06b-46d8-b49a-a49397547e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PosEncoding, forward2 time: 0.0031812191009521484\n",
      "CPU times: user 8.88 ms, sys: 3.81 ms, total: 12.7 ms\n",
      "Wall time: 3.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_var = PosEnc.forward2(torch.tensor(pos_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "fae792db-31ae-4378-8e05-6ac5ae507ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 30, 30]), torch.Size([128, 30]))"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extend_batch.shape, batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "1e109573-2b4f-4b86-b1aa-a06c9e1ffc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.13287878036499023\n"
     ]
    }
   ],
   "source": [
    "a= PropsEmb.forward(extend_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "7bc9617d-d081-4405-b180-e7b3b9eba3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.4492971897125244\n"
     ]
    }
   ],
   "source": [
    "b= PropsEmb.forward2(extend_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "f305013e-3bfb-4c55-824d-bd603a2c06ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.09531116485595703\n"
     ]
    }
   ],
   "source": [
    "c= PropsEmb.forward3(batch[:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "id": "31f2825d-2eff-49ff-967b-d1ecd7fd34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1: 0.0303800106048584\n",
      "time2: 0.01830887794494629\n",
      "time3: 0.19798922538757324\n",
      "time4: 0.0001437664031982422\n",
      "time: 0.24683094024658203\n"
     ]
    }
   ],
   "source": [
    "a = TokensEmb.forward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "65d717f9-bcf2-4db0-8aa0-827e28c9b1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time1: 1.5422999858856201\n",
      "time: 2.8998541831970215\n"
     ]
    }
   ],
   "source": [
    "b = TokensEmb.forward2(extend_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "d5f644c3-7b13-410e-9fdc-a0aaddc1de39",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randint() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [644]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: randint() received an invalid combination of arguments - got (tuple), but expected one of:\n * (int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n * (int low, int high, tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "torch.randint((128, 30, 5*4, 30+50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd13a78-0b5f-4a2e-8182-131c0d56f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_Size x SeqLen x Cnt_Meanings * CntCats x (CatEmb + MeaningEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "1f943382-226e-44f4-8e7d-2156f614d715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 643,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CAT_SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "48304bc3-5ed1-450b-9f10-074304a3f3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.rand((1, 128, 30, 5*4, 30+50)).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "id": "71db6576-e3b0-4e20-8970-de3f7ac0a0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 270 ms, sys: 404 ms, total: 674 ms\n",
      "Wall time: 149 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "c = a.repeat(30, 1,1,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "id": "abc71abb-d318-468c-b734-188178e54dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 980 µs, sys: 4.01 ms, total: 4.99 ms\n",
      "Wall time: 4.37 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d = einops.repeat(a, 'b h w s k -> (repeat b) h w s k', repeat=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "id": "6552682e-b501-43c9-b815-6ea23bf3dc55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatBackward0 at 0x7f9909b98340>"
      ]
     },
     "execution_count": 668,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "42f05593-8e75-407f-b254-0e13fcaa7c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ReshapeAliasBackward0 at 0x7f9909bd2520>"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "id": "0e5e3f30-5816-42ee-89e8-4cce519fd3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "a9c332c7-8a7e-42d1-b400-b9da7c427e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/Users/u14510182/Documents/Wiki_103/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "4ac571c0-852e-4499-9e76-9f2033c96ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30, 30, 20, 120])"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_tokens_ems.repeat(30,1,1,1,1).permute(1,0,2,3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "fb278919-adf0-44ed-941b-54754849b173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 128, 30, 40])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs.repeat(20,1,1,1).transpose(0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6844ef83-7b72-4df5-872b-f8c4d7f3ce3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.repeat(30,1,1,1,1).permute(1,2,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "d8c3db83-c000-4192-a7eb-e6c356299bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30, 40, 1])"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "67c7b181-bec1-4c0d-9bdc-601fcd5a07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = props_embs.repeat(20,1,1,1).permute(1,2,0,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "c0232fda-e29f-42e0-8461-7d8ab3814147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 30, 20, 40])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "ae72c0ee-e898-4379-809d-2ff660ebb34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0372, -0.3625,  0.1196, -0.6602, -0.5109, -0.3645,  0.4461,  0.4146,\n",
       "        -0.3136, -0.0255,  0.3199,  0.2844, -0.4189,  0.2136,  0.3882, -0.0892,\n",
       "         0.0270,  0.1638,  0.4387,  0.6790,  0.2568,  0.5872, -0.1455,  0.5291,\n",
       "        -0.1140,  0.0748,  0.6403, -0.6560, -0.4452, -0.1790, -0.2137, -0.1390,\n",
       "        -0.6755, -0.4683, -0.2915,  0.0262,  0.2795,  0.4243, -0.4794, -0.3079],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[1, 20, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "2dc4a0fc-30d0-40ec-8135-12f48eb8b049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False, False, False, False,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs[1, 8, :] == props_embs[1, 9, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "647dac98-c2f2-49f4-836c-0ad54b8affa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(30)\n",
      "1 tensor(40)\n",
      "2 tensor(40)\n",
      "3 tensor(40)\n",
      "4 tensor(10)\n",
      "5 tensor(0)\n",
      "6 tensor(30)\n",
      "7 tensor(30)\n",
      "8 tensor(30)\n",
      "9 tensor(40)\n",
      "10 tensor(40)\n",
      "11 tensor(40)\n",
      "12 tensor(40)\n",
      "13 tensor(20)\n",
      "14 tensor(10)\n",
      "15 tensor(30)\n",
      "16 tensor(40)\n",
      "17 tensor(40)\n",
      "18 tensor(40)\n",
      "19 tensor(40)\n",
      "20 tensor(40)\n",
      "21 tensor(40)\n",
      "22 tensor(40)\n",
      "23 tensor(40)\n",
      "24 tensor(40)\n",
      "25 tensor(30)\n",
      "26 tensor(30)\n",
      "27 tensor(30)\n",
      "28 tensor(30)\n"
     ]
    }
   ],
   "source": [
    "for i in range(29):\n",
    "    print(i, sum(props_embs[1, i, :] == props_embs[1, i+1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7d434501-591f-45a1-9a32-c81cc6582c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0372, -0.3625,  0.1196, -0.6602, -0.5109, -0.3645,  0.4461,  0.4146,\n",
       "        -0.3136, -0.0255,  0.3199,  0.2844, -0.4189,  0.2136,  0.3882, -0.0892,\n",
       "         0.0270,  0.1638,  0.4387,  0.6790,  0.2568,  0.5872, -0.1455,  0.5291,\n",
       "        -0.1140,  0.0748,  0.6403, -0.6560, -0.4452, -0.1790, -0.2137, -0.1390,\n",
       "        -0.6755, -0.4683, -0.2915,  0.0262,  0.2795,  0.4243, -0.4794, -0.3079],\n",
       "       dtype=torch.float64, grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props_embs[1, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683b35c-a8a6-4b3e-9c81-cfdcfaec91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22a0654-0fa0-4c27-ac50-25a9a57cf7cb",
   "metadata": {},
   "source": [
    "1) batch[:,:,0] -> Token_Emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e1ade9-862e-40ec-a260-2de141d4c130",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Arhitecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f12923-9fbc-4713-8a46-e3a72c984d9c",
   "metadata": {},
   "source": [
    "#### Примерный план\n",
    "1) Каким то образом получаем батч из числовых последовательностей токенов = [Batch_Size, MAX_SEQ_LEN]\n",
    "2) Определяем конец батча\n",
    "3) Определяем целевой токен для каждой строки батча\n",
    "    Получаем следующие данные W0 = [Enc_Pos_0, Token_0], W_Other = [Enc_Pos, Token]\n",
    "4) Рандомом из батча выбираем где мы будем учитывать \n",
    "\n",
    "\n",
    "Рассмотрим на примере одного \"предложения\".\n",
    "Предложение -> Tokens -> Seq (Seq_length может быть не равна MAX_SEQ_LEN)\n",
    "Строим матрицу Seq_Tokens = [Seq] * Seq_length.\n",
    "Так же берем матрицу расстояний = [MAX_SEQ_LEN, MAX_SEQ_LEN, POS_EMB_SIZE]\n",
    "Из нее извлекаем подматрицу Seq_Pos = [Seq_length, Seq_length, POS_EMB_SIZE]\n",
    "\n",
    "Получается две матрицы - Seq_Tokens и Seq_Pos\n",
    "Выбираем целевое слово, точнее его порядковый номер.\n",
    "Перемещаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94eb2f-778e-40ca-8961-02bcc373d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89ab45-c477-4492-aa81-a33b4a92e920",
   "metadata": {},
   "source": [
    "Можно так же подавать неправильные токены в последовательность, с целью чтоб они все равно предсказывали верные.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18fb676-da1f-40b9-a81f-bdba88229de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = [token_0, token_1, token_2, token_3]\n",
    "all_seq = [\n",
    "    [token_0, token_1, token_2, token_3],\n",
    "    [mask, token_1, token_2, token_3],\n",
    "    [token_0, mask, token_2, token_3],\n",
    "    [token_0, token_1, mask, token_3],\n",
    "    [token_0, token_1, token_2, mask],\n",
    "]\n",
    "\n",
    "for seq in all_seq:\n",
    "    seq_emb = []\n",
    "    mask_pos = get_mask_pos(seq)\n",
    "    \n",
    "    \n",
    "    for token_pos, token in enumerate(seq):\n",
    "        if token != '[MASK]':\n",
    "            token_info = []\n",
    "            pos_emb = get_pos_encoding(token_pos, mask_pos)\n",
    "            for cat in range(CNT_CATS):\n",
    "                emb_meanings = get_emb(token, cat) # Cnt_Meanings x Emb_Size\n",
    "                cat_emb = CATS[cat]\n",
    "                for m in range(Cnt_Meanings):\n",
    "                    token_info.append(torch.cat([pos_emb, cat_emb, emb_meaning]))\n",
    "                    \n",
    "            token_info = torch.cat(token_info) # Большой тензор, в котором заенкожена вся информация по токену\n",
    "            seq_emb.append(token_info)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf4db3e-009f-4ac7-b834-151436408779",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encod_matrix = gen_pos_encodind(MAX_LEN_SEQ)\n",
    "input_seq = [token_0, token_1, token_2, token_3]\n",
    "seq_emb = []\n",
    "\n",
    "for token_pos, token in enumerate(seq):\n",
    "    token_info = []\n",
    "    pos_emb = get_pos_encoding(token_pos, mask_pos)\n",
    "    for cat in range(CNT_CATS):\n",
    "        emb_meanings = get_emb(token, cat) # Cnt_Meanings x Emb_Size\n",
    "        cat_emb = CATS[cat]\n",
    "        for m in range(Cnt_Meanings):\n",
    "            token_info.append(torch.cat([pos_emb, cat_emb, emb_meaning]))\n",
    "\n",
    "    token_info = torch.cat(token_info) # Большой тензор, в котором заенкожена вся информация по токену\n",
    "    seq_emb.append(token_info)\n",
    "    \n",
    "\n",
    "variant = choose_variant() # Выбираеем вариант\n",
    "\n",
    "# Кличество вариаций обработки одной последовательности:\n",
    "# 1) Изменяем какое-либо слова:\n",
    "#      N вариантов выбрать изменяемое слово в последовательности.\n",
    "#      N вариантов выбрать целевое слово в последовательности.\n",
    "#      N^2 вариантов, если мы еще добавим опцию опускать(маскировать) ли целевое слово во время агрегации то 2 * N^2\n",
    "# 2) Не изменяем слова в последовательности:\n",
    "#      N вариантов выбрать целевое слово.\n",
    "#      2 - опускаем или нет целевое слово при аггрегации\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be1093-07a8-49f8-a7e1-06c5be7b2eae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad555dc3-3006-4ae8-b065-e3edc7a00115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8dcb63-60a4-4f71-b508-c6ecb0d89a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d359c-dff0-4ae4-baa3-ddaf22d2acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072cbea-163e-409d-a087-f13af5efce19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f38dd-7771-4686-a967-9a2cf7ef7317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5728e85-0b93-4f59-8efc-265d9ff14da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb213974-c4e9-4fa8-8a8f-6832c3db3958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ded5a5-b823-4842-9f45-b700ef2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
